{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a61ff1fd",
   "metadata": {},
   "source": [
    "# Day 3: Vector Embeddings, Vector DBs, and RAG Basics\n",
    "\n",
    "**Learning Goals:**\n",
    "- Understand what embeddings are and why they matter\n",
    "- Learn about vector databases (ChromaDB, FAISS)\n",
    "- Build your first RAG (Retrieval Augmented Generation) system\n",
    "- Implement semantic search over documents\n",
    "\n",
    "**Time:** 2-3 hours\n",
    "\n",
    "---\n",
    "\n",
    "## What We'll Build Today\n",
    "\n",
    "1. **Embeddings 101**: Convert text to vectors\n",
    "2. **Vector Store**: Store and search embeddings with ChromaDB\n",
    "3. **Document Loading**: Ingest and split documents\n",
    "4. **Simple RAG**: Query your own documents using LLM + retrieval\n",
    "5. **Semantic Search**: Find relevant info without exact keyword matches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1139b210",
   "metadata": {},
   "source": [
    "## Part 1: Environment Setup\n",
    "\n",
    "Load API keys and initialize the LLM (same as previous days)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c875b76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ OpenRouter key loaded\n",
      "‚úÖ LangSmith tracing enabled\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "OPENROUTER_API_KEY = os.getenv(\"OPENROUTER_API_KEY\")\n",
    "LANGCHAIN_API_KEY  = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "LANGCHAIN_TRACING  = os.getenv(\"LANGCHAIN_TRACING_V2\", \"false\") == \"true\"\n",
    "\n",
    "print(\"‚úÖ OpenRouter key loaded\" if OPENROUTER_API_KEY else \"‚ö†Ô∏è  Missing OPENROUTER_API_KEY\")\n",
    "print(\"‚úÖ LangSmith tracing enabled\" if LANGCHAIN_TRACING else \"‚ÑπÔ∏è  LangSmith tracing disabled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66b56cfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ LLM ready\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"openai/gpt-3.5-turbo\",\n",
    "    openai_api_key=OPENROUTER_API_KEY,\n",
    "    openai_api_base=\"https://openrouter.ai/api/v1\",\n",
    "    temperature=0.7,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ LLM ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2202b8",
   "metadata": {},
   "source": [
    "## Part 2: Understanding Embeddings\n",
    "\n",
    "**What are embeddings?**\n",
    "- Embeddings convert text into numerical vectors (arrays of numbers)\n",
    "- Similar meanings = similar vectors (close in vector space)\n",
    "- Enable semantic search: find \"king\" when searching for \"monarch\"\n",
    "\n",
    "**Why do we need them?**\n",
    "- Traditional search: exact keyword matching (\"apple\" won't find \"fruit\")\n",
    "- Semantic search: understands meaning and context\n",
    "- Foundation of RAG systems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1189f5",
   "metadata": {},
   "source": [
    "### 2a. Creating Embeddings\n",
    "\n",
    "Let's create embeddings for some text using OpenAI's embedding model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "315525dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of texts: 4\n",
      "Embedding dimensions: 1536\n",
      "\n",
      "First embedding (truncated): [-0.05894222483038902, -0.03949085623025894, -0.04609081521630287, 0.039207689464092255, 0.035374049097299576, 0.010956370271742344, 0.018776126205921173, 0.026748355478048325, 0.019244439899921417, -0.04761555790901184]...\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# Initialize embedding model\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-small\",\n",
    "    openai_api_key=OPENROUTER_API_KEY,\n",
    "    openai_api_base=\"https://openrouter.ai/api/v1\",\n",
    ")\n",
    "\n",
    "# Create embeddings for sample texts\n",
    "texts = [\n",
    "    \"The cat sleeps on the couch\",\n",
    "    \"A feline rests on the sofa\",\n",
    "    \"The dog plays in the yard\",\n",
    "    \"Python is a programming language\",\n",
    "]\n",
    "\n",
    "# Generate embeddings\n",
    "text_embeddings = embeddings.embed_documents(texts)\n",
    "\n",
    "print(f\"Number of texts: {len(text_embeddings)}\")\n",
    "print(f\"Embedding dimensions: {len(text_embeddings[0])}\")\n",
    "print(f\"\\nFirst embedding (truncated): {text_embeddings[0][:10]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8579a5e4",
   "metadata": {},
   "source": [
    "### 3c. Deleting Data\n",
    "\n",
    "If you need to clear your vector database or delete specific documents:\n",
    "\n",
    "1. **Delete the entire collection**: Clears everything in that specific collection.\n",
    "2. **Delete specific documents**: Use document IDs to remove specific entries.\n",
    "3. **Delete persistent storage**: If you used a `persist_directory`, you can simply delete the folder from your file system.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc38932",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Delete the entire collection\n",
    "# vectorstore.delete_collection()\n",
    "\n",
    "# 2. Delete specific documents (if you have their IDs)\n",
    "# vectorstore.delete(ids=[\"id1\", \"id2\"])\n",
    "\n",
    "# 3. To completely reset ChromaDB (manually delete the folder):\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "# Paths we used in this notebook\n",
    "db_paths = [\"./chroma_db\", \"./chroma_db_notes\", \"./chroma_db_filtered\", \"./chroma_db_complete\"]\n",
    "\n",
    "def reset_dbs():\n",
    "    for path in db_paths:\n",
    "        if os.path.exists(path):\n",
    "            shutil.rmtree(path)\n",
    "            print(f\"Deleted {path}\")\n",
    "        else:\n",
    "            print(f\"{path} not found\")\n",
    "\n",
    "# Uncomment to reset all databases in this notebook\n",
    "# reset_dbs()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4211121",
   "metadata": {},
   "source": [
    "### 2b. Measuring Similarity\n",
    "\n",
    "Let's see how similar these embeddings are using cosine similarity.\n",
    "Similar texts should have higher similarity scores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67cfed05",
   "metadata": {},
   "source": [
    "## Part 3: Vector Databases\n",
    "\n",
    "**What is a vector database?**\n",
    "- Specialized database for storing and searching embeddings\n",
    "- Optimized for similarity search (find nearest neighbors)\n",
    "- Popular options: ChromaDB, FAISS, Pinecone, Weaviate\n",
    "\n",
    "**Today we'll use ChromaDB** (simple, local, no setup required)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab972b0",
   "metadata": {},
   "source": [
    "### 3a. Install ChromaDB\n",
    "\n",
    "First, let's install the required package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86cc5c45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ChromaDB version: 1.5.0\n"
     ]
    }
   ],
   "source": [
    "# Install chromadb (run once)\n",
    "# !pip install chromadb\n",
    "\n",
    "import chromadb\n",
    "print(f\"‚úÖ ChromaDB version: {chromadb.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3733853",
   "metadata": {},
   "source": [
    "### 3b. Create a Vector Store\n",
    "\n",
    "Let's create a vector store and add some documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d16b69d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Vector store created and persisted to ./chroma_db\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# Sample documents about different topics\n",
    "documents = [\n",
    "    Document(\n",
    "        page_content=\"LangChain is a framework for developing applications powered by language models.\",\n",
    "        metadata={\"source\": \"langchain_docs\", \"topic\": \"frameworks\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Vector databases store embeddings and enable semantic search capabilities.\",\n",
    "        metadata={\"source\": \"vector_db_guide\", \"topic\": \"databases\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"RAG stands for Retrieval Augmented Generation, combining retrieval with LLMs.\",\n",
    "        metadata={\"source\": \"rag_tutorial\", \"topic\": \"rag\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Python is a high-level programming language known for its simplicity.\",\n",
    "        metadata={\"source\": \"python_intro\", \"topic\": \"programming\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Machine learning models can be fine-tuned on specific datasets to improve performance.\",\n",
    "        metadata={\"source\": \"ml_guide\", \"topic\": \"machine_learning\"}\n",
    "    ),\n",
    "]\n",
    "\n",
    "# Create vector store with persistent storage\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=documents,\n",
    "    embedding=embeddings,\n",
    "    collection_name=\"day3_collection\",\n",
    "    persist_directory=\"./chroma_db\"  # This is where the data will be saved\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Vector store created and persisted to ./chroma_db\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a6f31e",
   "metadata": {},
   "source": [
    "### 3c. Semantic Search\n",
    "\n",
    "Now let's search for relevant documents using natural language queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b5a0732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: 'What is a framework for building AI applications?'\n",
      "\n",
      "Top 3 Results:\n",
      "============================================================\n",
      "\n",
      "1. LangChain is a framework for developing applications powered by language models.\n",
      "   Source: langchain_docs | Topic: frameworks\n",
      "\n",
      "2. Machine learning models can be fine-tuned on specific datasets to improve performance.\n",
      "   Source: ml_guide | Topic: machine_learning\n",
      "\n",
      "3. Vector databases store embeddings and enable semantic search capabilities.\n",
      "   Source: vector_db_guide | Topic: databases\n"
     ]
    }
   ],
   "source": [
    "# Search for relevant documents\n",
    "query = \"What is a framework for building AI applications?\"\n",
    "\n",
    "results = vectorstore.similarity_search(query, k=3)\n",
    "\n",
    "print(f\"Query: '{query}'\\n\")\n",
    "print(\"Top 3 Results:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, doc in enumerate(results, 1):\n",
    "    print(f\"\\n{i}. {doc.page_content}\")\n",
    "    print(f\"   Source: {doc.metadata['source']} | Topic: {doc.metadata['topic']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7e2cc8",
   "metadata": {},
   "source": [
    "### 3d. Search with Scores\n",
    "\n",
    "Let's see the similarity scores to understand how relevant each result is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89f40185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: 'How do I store vectors?'\n",
      "\n",
      "Results with Similarity Scores:\n",
      "============================================================\n",
      "\n",
      "1. Score: 0.9301\n",
      "   Vector databases store embeddings and enable semantic search capabilities.\n",
      "   Source: vector_db_guide\n",
      "\n",
      "2. Score: 1.5569\n",
      "   RAG stands for Retrieval Augmented Generation, combining retrieval with LLMs.\n",
      "   Source: rag_tutorial\n",
      "\n",
      "3. Score: 1.6893\n",
      "   Machine learning models can be fine-tuned on specific datasets to improve performance.\n",
      "   Source: ml_guide\n"
     ]
    }
   ],
   "source": [
    "# Search with similarity scores\n",
    "query = \"How do I store vectors?\"\n",
    "\n",
    "results_with_scores = vectorstore.similarity_search_with_score(query, k=3)\n",
    "\n",
    "print(f\"Query: '{query}'\\n\")\n",
    "print(\"Results with Similarity Scores:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, (doc, score) in enumerate(results_with_scores, 1):\n",
    "    print(f\"\\n{i}. Score: {score:.4f}\")\n",
    "    print(f\"   {doc.page_content}\")\n",
    "    print(f\"   Source: {doc.metadata['source']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2de138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a collection with Cosine Similarity\n",
    "vectorstore_cosine = Chroma.from_documents(\n",
    "    documents=documents,\n",
    "    embedding=embeddings,\n",
    "    collection_name=\"cosine_collection\",\n",
    "    persist_directory=\"./chroma_db_cosine\",\n",
    "    collection_metadata={\"hnsw:space\": \"cosine\"} # <--- This tells Chroma to use Cosine\n",
    ")\n",
    "\n",
    "# Test search with cosine\n",
    "query = \"How do I store vectors?\"\n",
    "results_cosine = vectorstore_cosine.similarity_search_with_score(query, k=3)\n",
    "\n",
    "print(f\"Query: '{query}' (using Cosine Similarity)\")\n",
    "print(\"=\" * 60)\n",
    "for i, (doc, score) in enumerate(results_cosine, 1):\n",
    "    # Note: In Chroma's cosine path, it returns 1.0 - similarity \n",
    "    # so LOWER is still BETTER (closer to 0.0)\n",
    "    print(f\"{i}. Score: {score:.4f} | {doc.page_content[:60]}...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972dd817",
   "metadata": {},
   "source": [
    "### 3e. Understanding Distance Metrics\n",
    "\n",
    "By default, Chroma uses **Squared L2 (Euclidean) distance** where:\n",
    "- **Lower score = More similar**\n",
    "- **0.0** is an exact match.\n",
    "\n",
    "If you prefer **Cosine Similarity** (where you might expect higher scores for similarity), you have to tell Chroma when you create the collection using `collection_metadata`.\n",
    "\n",
    "**Supported spaces:**\n",
    "- `l2`: Squared L2 (default)\n",
    "- `ip`: Inner Product\n",
    "- `cosine`: Cosine Similarity\n",
    "\n",
    "Here's how to initialize with Cosine Similarity:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6698b70a",
   "metadata": {},
   "source": [
    "## Part 4: Document Loading and Text Splitting\n",
    "\n",
    "Real-world RAG systems need to handle long documents. Let's learn how to:\n",
    "1. Load documents from various sources\n",
    "2. Split them into manageable chunks\n",
    "3. Store them in a vector database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827f4d69",
   "metadata": {},
   "source": [
    "### 4a. Text Splitting Basics\n",
    "\n",
    "Why split documents?\n",
    "- LLMs have token limits\n",
    "- Smaller chunks = more precise retrieval\n",
    "- Better context management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b75be98b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text length: 911 characters\n",
      "Number of chunks: 7\n",
      "\n",
      "Chunk 1: 110 chars\n",
      "LangChain is a framework designed to simplify the creation of applications using large language mode...\n",
      "\n",
      "Chunk 2: 130 chars\n",
      "It provides a standard interface for chains, lots of integrations with other tools, and end-to-end c...\n",
      "\n",
      "Chunk 3: 160 chars\n",
      "The main value propositions of LangChain are:\n",
      "1. Components: Abstractions for working with LMs, alon...\n",
      "\n",
      "Chunk 4: 107 chars\n",
      "2. Off-the-shelf chains: Structured assemblies of components for accomplishing specific higher-level...\n",
      "\n",
      "Chunk 5: 96 chars\n",
      "LangChain makes it easy to build complex applications by chaining together different components....\n",
      "\n",
      "Chunk 6: 129 chars\n",
      "For example, you can chain together a prompt template, an LLM, and an output parser to create a simp...\n",
      "\n",
      "Chunk 7: 166 chars\n",
      "Memory is another crucial component that allows your application to remember previous interactions.\n",
      "...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Sample long text\n",
    "long_text = \"\"\"\n",
    "LangChain is a framework designed to simplify the creation of applications using large language models (LLMs). \n",
    "It provides a standard interface for chains, lots of integrations with other tools, and end-to-end chains for common applications.\n",
    "\n",
    "The main value propositions of LangChain are:\n",
    "1. Components: Abstractions for working with LMs, along with a collection of implementations for each abstraction.\n",
    "2. Off-the-shelf chains: Structured assemblies of components for accomplishing specific higher-level tasks.\n",
    "\n",
    "LangChain makes it easy to build complex applications by chaining together different components. \n",
    "For example, you can chain together a prompt template, an LLM, and an output parser to create a simple question-answering system.\n",
    "\n",
    "Memory is another crucial component that allows your application to remember previous interactions.\n",
    "This is essential for chatbots and conversational AI applications.\n",
    "\"\"\"\n",
    "\n",
    "# Create text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=200,        # Max characters per chunk\n",
    "    chunk_overlap=50,      # Overlap between chunks (preserves context)\n",
    "    length_function=len,\n",
    ")\n",
    "\n",
    "# Split the text\n",
    "chunks = text_splitter.create_documents([long_text])\n",
    "\n",
    "print(f\"Original text length: {len(long_text)} characters\")\n",
    "print(f\"Number of chunks: {len(chunks)}\\n\")\n",
    "\n",
    "for i, chunk in enumerate(chunks, 1):\n",
    "    print(f\"Chunk {i}: {len(chunk.page_content)} chars\")\n",
    "    print(f\"{chunk.page_content[:100]}...\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1d2756",
   "metadata": {},
   "source": [
    "### 4b. Loading Documents from Files\n",
    "\n",
    "LangChain provides loaders for many file types. Let's create a sample text file and load it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9703c8f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Sample document created at ../docs/learning_notes.txt\n"
     ]
    }
   ],
   "source": [
    "# Create a sample document\n",
    "sample_doc = \"\"\"\n",
    "# Personal AI Learning Journey - Week 1\n",
    "\n",
    "## Day 1: Python and LangChain Basics\n",
    "Today I learned the fundamentals of Python coming from a JavaScript background. \n",
    "Key concepts included snake_case naming, list comprehensions, and dictionary operations.\n",
    "I built my first LangChain application - a simple chatbot using ChatOpenAI.\n",
    "\n",
    "## Day 2: Chains and Memory\n",
    "Explored different chain types including sequential chains and router chains.\n",
    "Learned about prompt templates and how to use them effectively.\n",
    "Implemented conversation memory using ConversationBufferMemory and MessageHistory.\n",
    "\n",
    "## Day 3: Vector Embeddings and RAG\n",
    "Understanding embeddings and vector databases.\n",
    "Building a retrieval augmented generation system.\n",
    "Learning to work with ChromaDB for semantic search.\n",
    "\"\"\"\n",
    "\n",
    "# Save to file\n",
    "with open(\"../docs/learning_notes.txt\", \"w\") as f:\n",
    "    f.write(sample_doc)\n",
    "\n",
    "print(\"‚úÖ Sample document created at ../docs/learning_notes.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5400562d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1 document(s)\n",
      "\n",
      "Document content preview:\n",
      "\n",
      "# Personal AI Learning Journey - Week 1\n",
      "\n",
      "## Day 1: Python and LangChain Basics\n",
      "Today I learned the fundamentals of Python coming from a JavaScript background. \n",
      "Key concepts included snake_case naming ...\n",
      "\n",
      "Metadata: {'source': '../docs/learning_notes.txt'}\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "# Load the document\n",
    "loader = TextLoader(\"../docs/learning_notes.txt\")\n",
    "docs = loader.load()\n",
    "\n",
    "print(f\"Loaded {len(docs)} document(s)\")\n",
    "print(f\"\\nDocument content preview:\")\n",
    "print(docs[0].page_content[:200], \"...\\n\")\n",
    "print(f\"Metadata: {docs[0].metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27da6447",
   "metadata": {},
   "source": [
    "### 4c. Split and Store in Vector Database\n",
    "\n",
    "Now let's split the loaded document and store it in our vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "63a7d16e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split into 4 chunks\n",
      "\n",
      "‚úÖ Documents stored in ./chroma_db_notes\n"
     ]
    }
   ],
   "source": [
    "# Split the loaded document\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=300,\n",
    "    chunk_overlap=50,\n",
    ")\n",
    "\n",
    "split_docs = text_splitter.split_documents(docs)\n",
    "\n",
    "print(f\"Split into {len(split_docs)} chunks\\n\")\n",
    "\n",
    "# Create a new vector store with these documents\n",
    "vectorstore_notes = Chroma.from_documents(\n",
    "    documents=split_docs,\n",
    "    embedding=embeddings,\n",
    "    collection_name=\"learning_notes\",\n",
    "    persist_directory=\"./chroma_db_notes\"\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Documents stored in ./chroma_db_notes\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1eedf80",
   "metadata": {},
   "source": [
    "## Part 5: Building a Simple RAG System\n",
    "\n",
    "Now for the exciting part! Let's build a complete RAG system that:\n",
    "1. Retrieves relevant documents based on a query\n",
    "2. Passes them to an LLM as context\n",
    "3. Generates an answer grounded in those documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3699743a",
   "metadata": {},
   "source": [
    "### 5a. Basic RAG Chain\n",
    "\n",
    "We'll use LangChain's RetrievalQA chain to build our RAG system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7d5e8ebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ RAG chain created\n"
     ]
    }
   ],
   "source": [
    "from langchain_classic.chains import RetrievalQA\n",
    "\n",
    "# Create a retriever from our vector store\n",
    "retriever = vectorstore_notes.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 3}  # Return top 3 relevant chunks\n",
    ")\n",
    "\n",
    "# Create RAG chain\n",
    "rag_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",  # \"stuff\" means: stuff all docs into prompt\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True,  # Include sources in response\n",
    ")\n",
    "\n",
    "print(\"‚úÖ RAG chain created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e92d3f1",
   "metadata": {},
   "source": [
    "### 5b. Ask Questions!\n",
    "\n",
    "Let's ask questions about our learning journey."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "132d2f30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What did I learn on Day 2?\n",
      "\n",
      "Answer: On Day 2, you explored different chain types such as sequential chains and router chains. You also learned about prompt templates and how to use them effectively. Additionally, you implemented conversation memory using ConversationBufferMemory and MessageHistory.\n",
      "\n",
      "============================================================\n",
      "Source Documents:\n",
      "\n",
      "1. ## Day 2: Chains and Memory\n",
      "Explored different chain types including sequential chains and router chains.\n",
      "Learned about prompt templates and how to us...\n",
      "\n",
      "2. ## Day 1: Python and LangChain Basics\n",
      "Today I learned the fundamentals of Python coming from a JavaScript background. \n",
      "Key concepts included snake_cas...\n",
      "\n",
      "3. ## Day 3: Vector Embeddings and RAG\n",
      "Understanding embeddings and vector databases.\n",
      "Building a retrieval augmented generation system.\n",
      "Learning to work ...\n"
     ]
    }
   ],
   "source": [
    "# Ask a question\n",
    "query = \"What did I learn on Day 2?\"\n",
    "\n",
    "result = rag_chain.invoke({\"query\": query})\n",
    "\n",
    "print(f\"Question: {query}\\n\")\n",
    "print(f\"Answer: {result['result']}\\n\")\n",
    "print(\"=\" * 60)\n",
    "print(\"Source Documents:\")\n",
    "for i, doc in enumerate(result['source_documents'], 1):\n",
    "    print(f\"\\n{i}. {doc.page_content[:150]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a051d61e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is LangChain used for?\n",
      "\n",
      "Answer: LangChain is used for building conversational AI applications and systems. It allows developers to create chatbots, virtual assistants, and other natural language processing applications by utilizing different chain types, memory features, vector embeddings, and retrieval augmented generation systems. The framework provides tools for processing language input, generating responses, and managing conversation flow effectively.\n",
      "\n",
      "============================================================\n",
      "Source Documents:\n",
      "\n",
      "1. ## Day 1: Python and LangChain Basics\n",
      "Today I learned the fundamentals of Python coming from a JavaScript background. \n",
      "Key concepts included snake_cas...\n",
      "\n",
      "2. ## Day 2: Chains and Memory\n",
      "Explored different chain types including sequential chains and router chains.\n",
      "Learned about prompt templates and how to us...\n",
      "\n",
      "3. ## Day 3: Vector Embeddings and RAG\n",
      "Understanding embeddings and vector databases.\n",
      "Building a retrieval augmented generation system.\n",
      "Learning to work ...\n"
     ]
    }
   ],
   "source": [
    "# Try another question\n",
    "query = \"What is LangChain used for?\"\n",
    "\n",
    "result = rag_chain.invoke({\"query\": query})\n",
    "\n",
    "print(f\"Question: {query}\\n\")\n",
    "print(f\"Answer: {result['result']}\\n\")\n",
    "print(\"=\" * 60)\n",
    "print(\"Source Documents:\")\n",
    "for i, doc in enumerate(result['source_documents'], 1):\n",
    "    print(f\"\\n{i}. {doc.page_content[:150]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d93db6",
   "metadata": {},
   "source": [
    "### 5c. Custom RAG with Prompt Template\n",
    "\n",
    "Let's build a more customized RAG system with a custom prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "418efd94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Custom RAG chain created\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_classic.chains import RetrievalQA\n",
    "\n",
    "# Custom prompt template\n",
    "template = \"\"\"You are a helpful AI assistant answering questions about a learning journey.\n",
    "\n",
    "Use the following pieces of context to answer the question. \n",
    "If you don't know the answer based on the context, say so - don't make things up.\n",
    "Always cite which day or topic the information comes from.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer (be specific and cite sources):\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "# Create RAG chain with custom prompt\n",
    "custom_rag_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": prompt}\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Custom RAG chain created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "743f4f58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What topics are covered in the first three days?\n",
      "\n",
      "Answer: Day 1 covered Python basics such as snake_case naming, list comprehensions, and dictionary operations. Additionally, the day included building a simple chatbot using ChatOpenAI.\n",
      "\n",
      "Day 2 covered different chain types like sequential chains and router chains, prompt templates, and implementing conversation memory using ConversationBufferMemory and MessageHistory.\n",
      "\n",
      "Day 3 covered vector embeddings, building a retrieval augmented generation system, and working with ChromaDB for semantic search.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test custom RAG\n",
    "query = \"What topics are covered in the first three days?\"\n",
    "\n",
    "result = custom_rag_chain.invoke({\"query\": query})\n",
    "\n",
    "print(f\"Question: {query}\\n\")\n",
    "print(f\"Answer: {result['result']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92600e8a",
   "metadata": {},
   "source": [
    "## Part 6: Advanced Vector Store Features\n",
    "\n",
    "Let's explore some advanced features of vector stores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a563c19d",
   "metadata": {},
   "source": [
    "### 6a. Metadata Filtering\n",
    "\n",
    "Search only within specific metadata categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "798bca82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Vector store with metadata created at ./chroma_db_filtered\n"
     ]
    }
   ],
   "source": [
    "# Recreate our technical docs vectorstore\n",
    "tech_documents = [\n",
    "    Document(\n",
    "        page_content=\"LangChain is a framework for developing applications powered by language models.\",\n",
    "        metadata={\"source\": \"langchain_docs\", \"topic\": \"frameworks\", \"difficulty\": \"beginner\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Vector databases store embeddings and enable semantic search capabilities.\",\n",
    "        metadata={\"source\": \"vector_db_guide\", \"topic\": \"databases\", \"difficulty\": \"intermediate\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"RAG stands for Retrieval Augmented Generation, combining retrieval with LLMs.\",\n",
    "        metadata={\"source\": \"rag_tutorial\", \"topic\": \"rag\", \"difficulty\": \"intermediate\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Advanced RAG techniques include hypothetical document embeddings and multi-query retrieval.\",\n",
    "        metadata={\"source\": \"rag_advanced\", \"topic\": \"rag\", \"difficulty\": \"advanced\"}\n",
    "    ),\n",
    "]\n",
    "\n",
    "vectorstore_filtered = Chroma.from_documents(\n",
    "    documents=tech_documents,\n",
    "    embedding=embeddings,\n",
    "    collection_name=\"filtered_collection\",\n",
    "    persist_directory=\"./chroma_db_filtered\"\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Vector store with metadata created at ./chroma_db_filtered\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec619c73",
   "metadata": {},
   "source": [
    "### 6b. Maximum Marginal Relevance (MMR)\n",
    "\n",
    "MMR balances relevance with diversity - prevents returning very similar documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "31b8690f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: 'Tell me about RAG' (filtered by difficulty=intermediate)\n",
      "\n",
      "Results:\n",
      "\n",
      "1. RAG stands for Retrieval Augmented Generation, combining retrieval with LLMs.\n",
      "   Metadata: {'source': 'rag_tutorial', 'topic': 'rag', 'difficulty': 'intermediate'}\n",
      "\n",
      "2. Vector databases store embeddings and enable semantic search capabilities.\n",
      "   Metadata: {'source': 'vector_db_guide', 'topic': 'databases', 'difficulty': 'intermediate'}\n"
     ]
    }
   ],
   "source": [
    "# Search with metadata filter\n",
    "query = \"Tell me about RAG\"\n",
    "\n",
    "# Only search intermediate level documents\n",
    "results = vectorstore_filtered.similarity_search(\n",
    "    query,\n",
    "    k=3,\n",
    "    filter={\"difficulty\": \"intermediate\"}\n",
    ")\n",
    "\n",
    "print(f\"Query: '{query}' (filtered by difficulty=intermediate)\\n\")\n",
    "print(\"Results:\")\n",
    "for i, doc in enumerate(results, 1):\n",
    "    print(f\"\\n{i}. {doc.page_content}\")\n",
    "    print(f\"   Metadata: {doc.metadata}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "25f9c733",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard Similarity Search:\n",
      "1. Advanced RAG techniques include hypothetical document embeddings and multi-query...\n",
      "2. LangChain is a framework for developing applications powered by language models....\n",
      "3. Vector databases store embeddings and enable semantic search capabilities....\n",
      "\n",
      "============================================================\n",
      "\n",
      "MMR Search (diverse results):\n",
      "1. Advanced RAG techniques include hypothetical document embeddings and multi-query...\n",
      "2. LangChain is a framework for developing applications powered by language models....\n",
      "3. Vector databases store embeddings and enable semantic search capabilities....\n"
     ]
    }
   ],
   "source": [
    "# Search with MMR\n",
    "query = \"What are frameworks for AI?\"\n",
    "\n",
    "# Standard similarity search\n",
    "print(\"Standard Similarity Search:\")\n",
    "similarity_results = vectorstore_filtered.similarity_search(query, k=3)\n",
    "for i, doc in enumerate(similarity_results, 1):\n",
    "    print(f\"{i}. {doc.page_content[:80]}...\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "# MMR search (more diverse results)\n",
    "print(\"\\nMMR Search (diverse results):\")\n",
    "mmr_results = vectorstore_filtered.max_marginal_relevance_search(\n",
    "    query, \n",
    "    k=3,\n",
    "    fetch_k=10  # Fetch more candidates, then diversify\n",
    ")\n",
    "for i, doc in enumerate(mmr_results, 1):\n",
    "    print(f\"{i}. {doc.page_content[:80]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e948f74",
   "metadata": {},
   "source": [
    "## Part 7: Complete RAG Application\n",
    "\n",
    "Let's build a complete RAG application that can handle multiple document sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b7d6bb6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Knowledge base created and stored at ./chroma_db_complete\n"
     ]
    }
   ],
   "source": [
    "# Create a comprehensive knowledge base\n",
    "knowledge_base = [\n",
    "    # LangChain info\n",
    "    \"LangChain is a framework for building applications with large language models. It provides chains, agents, and memory systems.\",\n",
    "    \"LangChain supports multiple LLM providers including OpenAI, Anthropic, and open-source models.\",\n",
    "    \n",
    "    # Vector DB info\n",
    "    \"Vector databases like ChromaDB, Pinecone, and Weaviate store embeddings for semantic search.\",\n",
    "    \"Embeddings are numerical representations of text that capture semantic meaning.\",\n",
    "    \n",
    "    # RAG info\n",
    "    \"RAG (Retrieval Augmented Generation) combines information retrieval with language model generation.\",\n",
    "    \"RAG helps LLMs access external knowledge and reduces hallucinations.\",\n",
    "    \"The RAG process: 1) Embed query, 2) Retrieve relevant docs, 3) Generate answer with context.\",\n",
    "    \n",
    "    # Python info\n",
    "    \"Python is popular for AI development due to its simplicity and extensive ecosystem.\",\n",
    "    \"Key Python libraries for AI include LangChain, PyTorch, TensorFlow, and Hugging Face Transformers.\",\n",
    "]\n",
    "\n",
    "# Convert to documents\n",
    "kb_docs = [Document(page_content=text, metadata={\"source\": \"knowledge_base\"}) for text in knowledge_base]\n",
    "\n",
    "# Create vector store\n",
    "kb_vectorstore = Chroma.from_documents(\n",
    "    documents=kb_docs,\n",
    "    embedding=embeddings,\n",
    "    collection_name=\"complete_kb\",\n",
    "    persist_directory=\"./chroma_db_complete\"\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Knowledge base created and stored at ./chroma_db_complete\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1a7c16e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Conversational RAG system ready\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2_/vpdn53cj1bl7w9l8mgd5wnxr9bfspm/T/ipykernel_78609/2040164596.py:6: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory(\n"
     ]
    }
   ],
   "source": [
    "# Create a conversational RAG chain\n",
    "from langchain_classic.chains import ConversationalRetrievalChain\n",
    "from langchain_classic.memory import ConversationBufferMemory\n",
    "\n",
    "# Setup memory\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True,\n",
    "    output_key=\"answer\"\n",
    ")\n",
    "\n",
    "# Create conversational RAG\n",
    "conversational_rag = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=kb_vectorstore.as_retriever(search_kwargs={\"k\": 3}),\n",
    "    memory=memory,\n",
    "    return_source_documents=True,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Conversational RAG system ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "417b1e8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Q: What is RAG?\n",
      "============================================================\n",
      "A: RAG stands for Retrieval Augmented Generation. It is a process that combines information retrieval with language model generation to help access external knowledge and reduce hallucinations in language models. The RAG process involves embedding a query, retrieving relevant documents, and generating an answer with context.\n",
      "\n",
      "Sources: 3 documents retrieved\n",
      "\n",
      "============================================================\n",
      "Q: How does it help with LLMs?\n",
      "============================================================\n",
      "A: RAG (Retrieval Augmented Generation) helps with Language Model Generation by combining information retrieval with the generation process. It first embeds the query, then retrieves relevant documents, and finally generates answers with context. This process allows Language Models to access external knowledge, improve the quality of generated responses, and reduce hallucinations.\n",
      "\n",
      "Sources: 3 documents retrieved\n",
      "\n",
      "============================================================\n",
      "Q: What databases can I use for storing embeddings?\n",
      "============================================================\n",
      "A: Vector databases like ChromaDB, Pinecone, and Weaviate are commonly used for storing embeddings for semantic search.\n",
      "\n",
      "Sources: 3 documents retrieved\n"
     ]
    }
   ],
   "source": [
    "# Have a conversation!\n",
    "questions = [\n",
    "    \"What is RAG?\",\n",
    "    \"How does it help with LLMs?\",\n",
    "    \"What databases can I use for storing embeddings?\"\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Q: {question}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    result = conversational_rag.invoke({\"question\": question})\n",
    "    \n",
    "    print(f\"A: {result['answer']}\\n\")\n",
    "    print(f\"Sources: {len(result['source_documents'])} documents retrieved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6992da",
   "metadata": {},
   "source": [
    "## Part 8: Summary & Key Takeaways\n",
    "\n",
    "**What you learned today:**\n",
    "\n",
    "1. **Embeddings**: Convert text to vectors that capture semantic meaning\n",
    "2. **Vector Databases**: Store and search embeddings efficiently (ChromaDB)\n",
    "3. **Semantic Search**: Find relevant info based on meaning, not keywords\n",
    "4. **Document Processing**: Load and split documents into chunks\n",
    "5. **RAG Systems**: Retrieve relevant docs + generate answers with LLM\n",
    "6. **Advanced Features**: Metadata filtering, MMR, conversational RAG\n",
    "\n",
    "**Key Concepts:**\n",
    "- Embeddings enable semantic similarity search\n",
    "- Vector stores are optimized for finding similar embeddings\n",
    "- Text splitting is crucial for effective retrieval\n",
    "- RAG reduces hallucinations by grounding LLMs in real data\n",
    "- Conversational RAG maintains context across multiple turns\n",
    "\n",
    "**Tomorrow (Day 4):** Advanced RAG with multiple sources, citations, and reranking!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5469bb66",
   "metadata": {},
   "source": [
    "## üéØ Practice Exercises\n",
    "\n",
    "Try these on your own:\n",
    "\n",
    "1. **Create your own knowledge base**: Add 10-20 facts about your favorite topic and build a RAG system\n",
    "2. **Load a real file**: Use TextLoader or other loaders to ingest a PDF or markdown file\n",
    "3. **Experiment with chunk sizes**: Try different chunk_size and chunk_overlap values\n",
    "4. **Custom prompts**: Write your own RAG prompt template with a specific tone or format\n",
    "5. **Metadata filtering**: Create documents with custom metadata and search with filters\n",
    "\n",
    "**Bonus Challenge:**\n",
    "Build a \"personal notes assistant\" that can answer questions about all your learning notes from Days 1-3!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2170cf",
   "metadata": {},
   "source": [
    "## üìö Resources\n",
    "\n",
    "- [LangChain - Retrieval](https://python.langchain.com/docs/modules/data_connection/)\n",
    "- [ChromaDB Documentation](https://docs.trychroma.com/)\n",
    "- [Understanding Embeddings](https://platform.openai.com/docs/guides/embeddings)\n",
    "- [RAG Paper (original)](https://arxiv.org/abs/2005.11401)\n",
    "- [Vector Database Comparison](https://www.pinecone.io/learn/vector-database/)\n",
    "\n",
    "---\n",
    "\n",
    "**Day 3 Complete! üéâ**\n",
    "\n",
    "You now understand the fundamentals of RAG systems. Tomorrow we'll make them production-ready with advanced techniques!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626464c6",
   "metadata": {},
   "source": [
    "### 2b. Measuring Similarity\n",
    "\n",
    "Let's see how similar these embeddings are using cosine similarity.\n",
    "Similar texts should have higher similarity scores."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
