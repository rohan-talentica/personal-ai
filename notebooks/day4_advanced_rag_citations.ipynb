{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8fe6f158",
   "metadata": {},
   "source": [
    "# Day 4: Advanced RAG with Multiple Sources and Citations\n",
    "\n",
    "**Learning Goals:**\n",
    "- Query multiple document sources simultaneously\n",
    "- Implement source attribution and citations in responses\n",
    "- Use metadata filtering for targeted retrieval\n",
    "- Build a multi-document RAG system with proper citations\n",
    "- Learn reranking techniques for better retrieval\n",
    "- Understand retrieval quality metrics\n",
    "\n",
    "**Time:** 2-3 hours\n",
    "\n",
    "---\n",
    "\n",
    "## What We'll Build Today\n",
    "\n",
    "1. **Multi-Source RAG**: Query different document collections\n",
    "2. **Source Attribution**: Track which documents were used in answers\n",
    "3. **Metadata Filtering**: Search specific document types or authors\n",
    "4. **Citation System**: Add proper citations to LLM responses\n",
    "5. **Reranking**: Improve retrieval accuracy with reranking\n",
    "6. **Retrieval Evaluation**: Measure retrieval quality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d298a1c4",
   "metadata": {},
   "source": [
    "## Part 1: Environment Setup\n",
    "\n",
    "Load API keys and initialize components (same as previous days)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0971c6dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… OpenRouter key loaded\n",
      "âœ… LangSmith tracing enabled\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "OPENROUTER_API_KEY = os.getenv(\"OPENROUTER_API_KEY\")\n",
    "LANGCHAIN_API_KEY  = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "LANGCHAIN_TRACING  = os.getenv(\"LANGCHAIN_TRACING_V2\", \"false\") == \"true\"\n",
    "\n",
    "print(\"âœ… OpenRouter key loaded\" if OPENROUTER_API_KEY else \"âš ï¸  Missing OPENROUTER_API_KEY\")\n",
    "print(\"âœ… LangSmith tracing enabled\" if LANGCHAIN_TRACING else \"â„¹ï¸  LangSmith tracing disabled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac7a6716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… LLM and Embeddings ready\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"openai/gpt-3.5-turbo\",\n",
    "    openai_api_key=OPENROUTER_API_KEY,\n",
    "    openai_api_base=\"https://openrouter.ai/api/v1\",\n",
    "    temperature=0.7,\n",
    ")\n",
    "\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-small\",\n",
    "    openai_api_key=OPENROUTER_API_KEY,\n",
    "    openai_api_base=\"https://openrouter.ai/api/v1\",\n",
    ")\n",
    "\n",
    "print(\"âœ… LLM and Embeddings ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d5509a",
   "metadata": {},
   "source": [
    "## Part 2: Building a Multi-Source Document Collection\n",
    "\n",
    "**Why Multiple Sources?**\n",
    "- Different document types (research papers, blog posts, documentation)\n",
    "- Different authors or perspectives\n",
    "- Time-based filtering (recent vs historical)\n",
    "- Domain-specific collections\n",
    "\n",
    "Let's create a diverse document collection with rich metadata."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc7c1a6",
   "metadata": {},
   "source": [
    "### 2a. Creating Documents with Rich Metadata\n",
    "\n",
    "Metadata is crucial for advanced RAG - it enables filtering, source attribution, and citations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed8e2108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Created 8 documents from 4 different source types\n",
      "ðŸ“š Document types: news, documentation, blog, paper\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.documents import Document\n",
    "from datetime import datetime\n",
    "\n",
    "# Create documents from different sources with rich metadata\n",
    "documents = [\n",
    "    # Research Papers on AI\n",
    "    Document(\n",
    "        page_content=\"Large Language Models (LLMs) like GPT-4 demonstrate emergent abilities in reasoning, comprehension, and generation. These models are trained on diverse internet text and can perform tasks they weren't explicitly trained for.\",\n",
    "        metadata={\n",
    "            \"source\": \"research_paper\",\n",
    "            \"title\": \"Emergent Abilities of Large Language Models\",\n",
    "            \"author\": \"Dr. Jane Smith\",\n",
    "            \"year\": 2023,\n",
    "            \"category\": \"artificial_intelligence\",\n",
    "            \"document_type\": \"paper\",\n",
    "            \"page\": 1\n",
    "        }\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Retrieval Augmented Generation (RAG) combines the power of large language models with external knowledge retrieval. This approach significantly reduces hallucinations and provides more accurate, up-to-date information.\",\n",
    "        metadata={\n",
    "            \"source\": \"research_paper\",\n",
    "            \"title\": \"RAG: Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\",\n",
    "            \"author\": \"Dr. John Doe\",\n",
    "            \"year\": 2023,\n",
    "            \"category\": \"artificial_intelligence\",\n",
    "            \"document_type\": \"paper\",\n",
    "            \"page\": 3\n",
    "        }\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Vector databases enable efficient similarity search over high-dimensional embeddings. Popular options include Pinecone, Weaviate, Qdrant, and ChromaDB, each offering different trade-offs in performance and features.\",\n",
    "        metadata={\n",
    "            \"source\": \"research_paper\",\n",
    "            \"title\": \"Vector Databases for AI Applications\",\n",
    "            \"author\": \"Dr. Jane Smith\",\n",
    "            \"year\": 2024,\n",
    "            \"category\": \"databases\",\n",
    "            \"document_type\": \"paper\",\n",
    "            \"page\": 2\n",
    "        }\n",
    "    ),\n",
    "    \n",
    "    # Technical Blog Posts\n",
    "    Document(\n",
    "        page_content=\"LangChain provides a comprehensive framework for building LLM applications. It includes modules for prompts, chains, agents, memory, and retrieval, making it easier to develop complex AI systems.\",\n",
    "        metadata={\n",
    "            \"source\": \"blog_post\",\n",
    "            \"title\": \"Getting Started with LangChain\",\n",
    "            \"author\": \"Tech Blogger\",\n",
    "            \"year\": 2024,\n",
    "            \"category\": \"software_engineering\",\n",
    "            \"document_type\": \"blog\",\n",
    "            \"url\": \"https://techblog.example/langchain-intro\"\n",
    "        }\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"When building production RAG systems, consider chunking strategies, embedding models, retrieval algorithms, and prompt engineering. Each choice significantly impacts the quality of your results.\",\n",
    "        metadata={\n",
    "            \"source\": \"blog_post\",\n",
    "            \"title\": \"Production RAG Best Practices\",\n",
    "            \"author\": \"ML Engineer\",\n",
    "            \"year\": 2024,\n",
    "            \"category\": \"software_engineering\",\n",
    "            \"document_type\": \"blog\",\n",
    "            \"url\": \"https://mlblog.example/rag-best-practices\"\n",
    "        }\n",
    "    ),\n",
    "    \n",
    "    # Documentation\n",
    "    Document(\n",
    "        page_content=\"ChromaDB is an open-source embedding database. It's designed to be simple, fast, and easy to use. Install with: pip install chromadb. Basic usage involves creating a client, creating a collection, and adding documents.\",\n",
    "        metadata={\n",
    "            \"source\": \"documentation\",\n",
    "            \"title\": \"ChromaDB Documentation - Quick Start\",\n",
    "            \"author\": \"ChromaDB Team\",\n",
    "            \"year\": 2024,\n",
    "            \"category\": \"databases\",\n",
    "            \"document_type\": \"documentation\",\n",
    "            \"version\": \"0.4.0\"\n",
    "        }\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"LangChain's RetrievalQA chain combines a retriever with a question-answering chain. It retrieves relevant documents and passes them to an LLM to generate answers based on the retrieved context.\",\n",
    "        metadata={\n",
    "            \"source\": \"documentation\",\n",
    "            \"title\": \"LangChain Documentation - RetrievalQA\",\n",
    "            \"author\": \"LangChain Team\",\n",
    "            \"year\": 2024,\n",
    "            \"category\": \"software_engineering\",\n",
    "            \"document_type\": \"documentation\",\n",
    "            \"version\": \"0.1.0\"\n",
    "        }\n",
    "    ),\n",
    "    \n",
    "    # News Articles\n",
    "    Document(\n",
    "        page_content=\"OpenAI announced GPT-4 Turbo with improved performance and reduced costs. The new model features a 128K context window and knowledge cutoff of April 2023, making it more capable for complex tasks.\",\n",
    "        metadata={\n",
    "            \"source\": \"news_article\",\n",
    "            \"title\": \"OpenAI Releases GPT-4 Turbo\",\n",
    "            \"author\": \"Tech News Reporter\",\n",
    "            \"year\": 2024,\n",
    "            \"category\": \"artificial_intelligence\",\n",
    "            \"document_type\": \"news\",\n",
    "            \"publication\": \"Tech Daily\"\n",
    "        }\n",
    "    ),\n",
    "]\n",
    "\n",
    "print(f\"âœ… Created {len(documents)} documents from {len(set(d.metadata['document_type'] for d in documents))} different source types\")\n",
    "print(f\"ðŸ“š Document types: {', '.join(set(d.metadata['document_type'] for d in documents))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222fff89",
   "metadata": {},
   "source": [
    "### 2b. Storing Documents in ChromaDB with Metadata\n",
    "\n",
    "Let's store these documents and preserve all metadata for later filtering and citation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "671c9838",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2_/vpdn53cj1bl7w9l8mgd5wnxr9bfspm/T/ipykernel_20976/1584831788.py:4: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the `langchain-chroma package and should be used instead. To use it run `pip install -U `langchain-chroma` and import as `from `langchain_chroma import Chroma``.\n",
      "  vectorstore = Chroma(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Stored 8 documents in ChromaDB\n",
      "ðŸ“Š Collection name: multi_source_docs\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "# Create a new ChromaDB collection for multi-source documents\n",
    "vectorstore = Chroma(\n",
    "    collection_name=\"multi_source_docs\",\n",
    "    embedding_function=embeddings,\n",
    "    persist_directory=\"./chroma_db_day4_sources\"\n",
    ")\n",
    "\n",
    "# Add documents with metadata\n",
    "vectorstore.add_documents(documents)\n",
    "\n",
    "print(f\"âœ… Stored {len(documents)} documents in ChromaDB\")\n",
    "print(f\"ðŸ“Š Collection name: multi_source_docs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1366bbcd",
   "metadata": {},
   "source": [
    "## Part 3: Metadata Filtering for Targeted Retrieval\n",
    "\n",
    "**Why Filter?**\n",
    "- Query specific document types (only papers, only docs)\n",
    "- Search by author, year, or category\n",
    "- Improve relevance by narrowing scope\n",
    "- Create specialized retrieval pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17da6331",
   "metadata": {},
   "source": [
    "### 3a. Filtering by Document Type\n",
    "\n",
    "Let's retrieve only from research papers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f282b0a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Search Results (Papers Only):\n",
      "\n",
      "1. Emergent Abilities of Large Language Models\n",
      "   Author: Dr. Jane Smith\n",
      "   Type: paper\n",
      "   Content: Large Language Models (LLMs) like GPT-4 demonstrate emergent abilities in reasoning, comprehension, ...\n",
      "\n",
      "2. RAG: Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\n",
      "   Author: Dr. John Doe\n",
      "   Type: paper\n",
      "   Content: Retrieval Augmented Generation (RAG) combines the power of large language models with external knowl...\n",
      "\n",
      "3. Vector Databases for AI Applications\n",
      "   Author: Dr. Jane Smith\n",
      "   Type: paper\n",
      "   Content: Vector databases enable efficient similarity search over high-dimensional embeddings. Popular option...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Search only research papers\n",
    "query = \"What are the benefits of using large language models?\"\n",
    "\n",
    "# Apply metadata filter\n",
    "results = vectorstore.similarity_search(\n",
    "    query,\n",
    "    k=3,\n",
    "    filter={\"document_type\": \"paper\"}\n",
    ")\n",
    "\n",
    "print(\"ðŸ” Search Results (Papers Only):\\n\")\n",
    "for i, doc in enumerate(results, 1):\n",
    "    print(f\"{i}. {doc.metadata['title']}\")\n",
    "    print(f\"   Author: {doc.metadata['author']}\")\n",
    "    print(f\"   Type: {doc.metadata['document_type']}\")\n",
    "    print(f\"   Content: {doc.page_content[:100]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f8f030",
   "metadata": {},
   "source": [
    "### 3b. Filtering by Multiple Criteria\n",
    "\n",
    "Combine multiple metadata filters for precise retrieval.\n",
    "\n",
    "**Note**: For multiple conditions, use `$and` operator:\n",
    "```python\n",
    "filter={\"$and\": [{\"document_type\": \"documentation\"}, {\"year\": 2024}]}\n",
    "```\n",
    "\n",
    "For OR conditions, use `$or`:\n",
    "```python\n",
    "filter={\"$or\": [{\"document_type\": \"paper\"}, {\"document_type\": \"blog\"}]}\n",
    "```\n",
    "\n",
    "For IN conditions, use `$in`:\n",
    "```python\n",
    "filter={\"document_type\": {\"$in\": [\"blog\", \"documentation\"]}}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ecb69f06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Search Results (2024 Documentation Only):\n",
      "\n",
      "1. ChromaDB Documentation - Quick Start\n",
      "   Type: documentation (2024)\n",
      "   Content: ChromaDB is an open-source embedding database. It's designed to be simple, fast, and easy to use. Install with: pip inst...\n",
      "\n",
      "2. LangChain Documentation - RetrievalQA\n",
      "   Type: documentation (2024)\n",
      "   Content: LangChain's RetrievalQA chain combines a retriever with a question-answering chain. It retrieves relevant documents and ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Search only recent documentation (2024)\n",
    "query = \"How do I use ChromaDB?\"\n",
    "\n",
    "results = vectorstore.similarity_search(\n",
    "    query,\n",
    "    k=2,\n",
    "    filter={\n",
    "    \"$and\": [\n",
    "        {\"document_type\": \"documentation\"},\n",
    "        {\"year\": 2024}\n",
    "    ]\n",
    "}\n",
    ")\n",
    "\n",
    "print(\"ðŸ” Search Results (2024 Documentation Only):\\n\")\n",
    "for i, doc in enumerate(results, 1):\n",
    "    print(f\"{i}. {doc.metadata['title']}\")\n",
    "    print(f\"   Type: {doc.metadata['document_type']} ({doc.metadata['year']})\")\n",
    "    print(f\"   Content: {doc.page_content[:120]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7c7a62",
   "metadata": {},
   "source": [
    "### 3c. Filtering by Author\n",
    "\n",
    "Find all documents from a specific author."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "71073cc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Search Results (Dr. Jane Smith's documents):\n",
      "\n",
      "1. Vector Databases for AI Applications\n",
      "   Year: 2024\n",
      "   Category: databases\n",
      "   Content: Vector databases enable efficient similarity search over high-dimensional embeddings. Popular option...\n",
      "\n",
      "2. Emergent Abilities of Large Language Models\n",
      "   Year: 2023\n",
      "   Category: artificial_intelligence\n",
      "   Content: Large Language Models (LLMs) like GPT-4 demonstrate emergent abilities in reasoning, comprehension, ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Search documents by Dr. Jane Smith\n",
    "query = \"vector databases\"\n",
    "\n",
    "results = vectorstore.similarity_search(\n",
    "    query,\n",
    "    k=3,\n",
    "    filter={\"author\": \"Dr. Jane Smith\"}\n",
    ")\n",
    "\n",
    "print(\"ðŸ” Search Results (Dr. Jane Smith's documents):\\n\")\n",
    "for i, doc in enumerate(results, 1):\n",
    "    print(f\"{i}. {doc.metadata['title']}\")\n",
    "    print(f\"   Year: {doc.metadata['year']}\")\n",
    "    print(f\"   Category: {doc.metadata['category']}\")\n",
    "    print(f\"   Content: {doc.page_content[:100]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3109b57c",
   "metadata": {},
   "source": [
    "## Part 4: Building a RAG System with Citations\n",
    "\n",
    "**Why Citations Matter?**\n",
    "- Transparency and trust\n",
    "- Fact verification\n",
    "- Follow-up reading\n",
    "- Academic/professional credibility"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3d69fd",
   "metadata": {},
   "source": [
    "### 4a. Simple RAG with Source Attribution\n",
    "\n",
    "First, let's build a basic RAG that includes sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1aa94576",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â“ Question: What is RAG and why is it useful?\n",
      "\n",
      "ðŸ“ Answer: RAG, or Retrieval Augmented Generation, is an approach that combines the power of large language models with external knowledge retrieval. This method significantly reduces hallucinations and provides more accurate, up-to-date information by leveraging both the capabilities of language models and external sources of knowledge. This approach is useful because it helps improve the quality of generated content by incorporating relevant information from external sources, thus enhancing the accuracy and reliability of the generated output.\n",
      "\n",
      "Sources used: The provided context snippets about RAG and LangChain.\n",
      "\n",
      "ðŸ“š Sources Used:\n",
      "\n",
      "1. RAG: Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\n",
      "   By Dr. John Doe (2023)\n",
      "   Type: paper\n",
      "\n",
      "2. Production RAG Best Practices\n",
      "   By ML Engineer (2024)\n",
      "   Type: blog\n",
      "\n",
      "3. LangChain Documentation - RetrievalQA\n",
      "   By LangChain Team (2024)\n",
      "   Type: documentation\n",
      "\n",
      "4. Getting Started with LangChain\n",
      "   By Tech Blogger (2024)\n",
      "   Type: blog\n"
     ]
    }
   ],
   "source": [
    "from langchain_classic.chains import RetrievalQA\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# Create a custom prompt that encourages source mention\n",
    "template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer: Provide a comprehensive answer and mention which sources you used.\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "# Create RetrievalQA chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectorstore.as_retriever(search_kwargs={\"k\": 4}),\n",
    "    chain_type_kwargs={\"prompt\": prompt},\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "# Query the system\n",
    "question = \"What is RAG and why is it useful?\"\n",
    "result = qa_chain.invoke({\"query\": question})\n",
    "\n",
    "print(\"â“ Question:\", question)\n",
    "print(\"\\nðŸ“ Answer:\", result['result'])\n",
    "print(\"\\nðŸ“š Sources Used:\")\n",
    "for i, doc in enumerate(result['source_documents'], 1):\n",
    "    print(f\"\\n{i}. {doc.metadata['title']}\")\n",
    "    print(f\"   By {doc.metadata['author']} ({doc.metadata['year']})\")\n",
    "    print(f\"   Type: {doc.metadata['document_type']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1696493a",
   "metadata": {},
   "source": [
    "### 4b. Advanced Citation System\n",
    "\n",
    "Now let's build a proper citation system with numbered references."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "00a1b698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“„ Formatted Context with Citations:\n",
      "\n",
      "[1] Vector databases enable efficient similarity search over high-dimensional embeddings. Popular options include Pinecone, Weaviate, Qdrant, and ChromaDB, each offering different trade-offs in performance and features.\n",
      "\n",
      "[2] Retrieval Augmented Generation (RAG) combines the power of large language models with external knowledge retrieval. This approach significantly reduces hallucinations and provides more accurate, up-to-date information.\n",
      "\n",
      "[3] When building production RAG systems, consider chun...\n",
      "\n",
      "======================================================================\n",
      "\n",
      "\n",
      "References:\n",
      "[1] Vector Databases for AI Applications - Dr. Jane Smith (2024), p. 2\n",
      "[2] RAG: Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks - Dr. John Doe (2023), p. 3\n",
      "[3] Production RAG Best Practices - ML Engineer (2024)\n",
      "    https://mlblog.example/rag-best-practices\n",
      "[4] Getting Started with LangChain - Tech Blogger (2024)\n",
      "    https://techblog.example/langchain-intro\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.documents import Document\n",
    "from typing import List\n",
    "\n",
    "def format_citations(documents: List[Document]) -> tuple[str, List[dict]]:\n",
    "    \"\"\"\n",
    "    Format documents with citation numbers and return formatted context + citation list\n",
    "    \"\"\"\n",
    "    citations = []\n",
    "    formatted_chunks = []\n",
    "    \n",
    "    for i, doc in enumerate(documents, 1):\n",
    "        # Create citation entry\n",
    "        citation = {\n",
    "            \"number\": i,\n",
    "            \"title\": doc.metadata.get(\"title\", \"Untitled\"),\n",
    "            \"author\": doc.metadata.get(\"author\", \"Unknown\"),\n",
    "            \"year\": doc.metadata.get(\"year\", \"N/A\"),\n",
    "            \"type\": doc.metadata.get(\"document_type\", \"unknown\"),\n",
    "        }\n",
    "        \n",
    "        # Add optional metadata\n",
    "        if \"page\" in doc.metadata:\n",
    "            citation[\"page\"] = doc.metadata[\"page\"]\n",
    "        if \"url\" in doc.metadata:\n",
    "            citation[\"url\"] = doc.metadata[\"url\"]\n",
    "        \n",
    "        citations.append(citation)\n",
    "        \n",
    "        # Format chunk with citation number\n",
    "        formatted_chunks.append(f\"[{i}] {doc.page_content}\")\n",
    "    \n",
    "    context = \"\\n\\n\".join(formatted_chunks)\n",
    "    return context, citations\n",
    "\n",
    "def format_citation_list(citations: List[dict]) -> str:\n",
    "    \"\"\"Format citations as a readable reference list\"\"\"\n",
    "    citation_text = \"\\n\\nReferences:\\n\"\n",
    "    for cite in citations:\n",
    "        cite_str = f\"[{cite['number']}] {cite['title']} - {cite['author']} ({cite['year']})\"\n",
    "        if 'page' in cite:\n",
    "            cite_str += f\", p. {cite['page']}\"\n",
    "        if 'url' in cite:\n",
    "            cite_str += f\"\\n    {cite['url']}\"\n",
    "        citation_text += f\"{cite_str}\\n\"\n",
    "    return citation_text\n",
    "\n",
    "# Test the citation system\n",
    "question = \"Explain how vector databases work in AI applications\"\n",
    "\n",
    "# Retrieve documents\n",
    "retrieved_docs = vectorstore.similarity_search(question, k=4)\n",
    "\n",
    "# Format with citations\n",
    "context, citations = format_citations(retrieved_docs)\n",
    "\n",
    "print(\"ðŸ“„ Formatted Context with Citations:\\n\")\n",
    "print(context[:500] + \"...\")\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(format_citation_list(citations))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6d315d",
   "metadata": {},
   "source": [
    "### 4c. Complete RAG Chain with Citations\n",
    "\n",
    "Integrate everything into a RAG chain that produces cited answers.\n",
    "\n",
    "**Python Tip**: The `**search_kwargs` syntax unpacks a dictionary as keyword arguments. It's not pointers - it's a way to dynamically pass parameters to functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab022d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â“ Question: What are the best practices for building production RAG systems?\n",
      "\n",
      "ðŸ“ Answer:\n",
      "The best practices for building production RAG systems include considering chunking strategies, embedding models, retrieval algorithms, and prompt engineering. Each of these choices significantly impacts the quality of the results obtained from the system [1].\n",
      "\n",
      "\n",
      "References:\n",
      "[1] Production RAG Best Practices - ML Engineer (2024)\n",
      "    https://mlblog.example/rag-best-practices\n",
      "[2] RAG: Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks - Dr. John Doe (2023), p. 3\n",
      "[3] Getting Started with LangChain - Tech Blogger (2024)\n",
      "    https://techblog.example/langchain-intro\n",
      "[4] LangChain Documentation - RetrievalQA - LangChain Team (2024)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "class CitedRAG:\n",
    "    \"\"\"RAG system that provides answers with proper citations\"\"\"\n",
    "    \n",
    "    def __init__(self, llm, vectorstore, k=4):\n",
    "        self.llm = llm\n",
    "        self.vectorstore = vectorstore\n",
    "        self.k = k\n",
    "        \n",
    "        # Prompt that expects citations\n",
    "        self.prompt = PromptTemplate(\n",
    "            template=\"\"\"You are a helpful assistant that provides accurate answers based on provided sources.\n",
    "Use the context below to answer the question. Each piece of context is marked with a citation number [1], [2], etc.\n",
    "\n",
    "When you use information from a source, cite it using the reference number in square brackets, like [1] or [2].\n",
    "You can cite multiple sources like [1,2] if information comes from multiple sources.\n",
    "\n",
    "If the context doesn't contain enough information to answer the question, say so clearly.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Provide a clear, well-cited answer:\"\"\",\n",
    "            input_variables=[\"context\", \"question\"]\n",
    "        )\n",
    "        \n",
    "        # Create LCEL chain (modern LangChain approach)\n",
    "        self.chain = (\n",
    "            {\"context\": RunnablePassthrough(), \"question\": RunnablePassthrough()}\n",
    "            | self.prompt\n",
    "            | self.llm\n",
    "            | StrOutputParser()\n",
    "        )\n",
    "    \n",
    "    def query(self, question: str, filter_dict: dict = None):\n",
    "        \"\"\"Query the RAG system and return answer with citations\"\"\"\n",
    "        \n",
    "        # Retrieve relevant documents\n",
    "        search_kwargs = {\"k\": self.k}\n",
    "        if filter_dict:\n",
    "            search_kwargs[\"filter\"] = filter_dict\n",
    "            \n",
    "        # **search_kwargs unpacks the dict as keyword arguments\n",
    "        # Equivalent to: similarity_search(question, k=4, filter=filter_dict)\n",
    "        retrieved_docs = self.vectorstore.similarity_search(\n",
    "            question, \n",
    "            **search_kwargs\n",
    "        )\n",
    "        \n",
    "        # Format with citations\n",
    "        context, citations = format_citations(retrieved_docs)\n",
    "        \n",
    "        # Generate answer\n",
    "        answer = self.chain.invoke({\"context\": context, \"question\": question})\n",
    "        \n",
    "        # Format final output\n",
    "        result = {\n",
    "            \"question\": question,\n",
    "            \"answer\": answer,\n",
    "            \"citations\": citations,\n",
    "            \"source_documents\": retrieved_docs\n",
    "        }\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def print_result(self, result):\n",
    "        \"\"\"Pretty print the result\"\"\"\n",
    "        print(\"â“ Question:\", result['question'])\n",
    "        print(\"\\nðŸ“ Answer:\")\n",
    "        print(result['answer'])\n",
    "        print(format_citation_list(result['citations']))\n",
    "\n",
    "# Create CitedRAG instance\n",
    "cited_rag = CitedRAG(llm, vectorstore, k=4)\n",
    "\n",
    "# Test it!\n",
    "question = \"What are the best practices for building production RAG systems?\"\n",
    "result = cited_rag.query(question)\n",
    "cited_rag.print_result(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf05cde9",
   "metadata": {},
   "source": [
    "### 4d. Query with Filters and Citations\n",
    "\n",
    "Combine metadata filtering with citations.\n",
    "\n",
    "**Filter Syntax Examples:**\n",
    "- Single filter: `{\"document_type\": \"paper\"}`\n",
    "- Multiple AND: `{\"$and\": [{\"document_type\": \"documentation\"}, {\"year\": 2024}]}`\n",
    "- OR condition: `{\"$or\": [{\"author\": \"Dr. Jane Smith\"}, {\"author\": \"Dr. John Doe\"}]}`\n",
    "- IN condition: `{\"document_type\": {\"$in\": [\"blog\", \"documentation\"]}}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2a219318",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â“ Question: How do I get started with LangChain?\n",
      "\n",
      "ðŸ“ Answer:\n",
      "To get started with LangChain, you can begin by exploring its comprehensive framework designed for building LLM applications. This framework includes modules for prompts, chains, agents, memory, and retrieval, which collectively simplify the development of complex AI systems [1]. By leveraging LangChain's RetrievalQA chain, which combines a retriever with a question-answering chain, you can retrieve relevant documents and pass them to an LLM to generate answers based on the retrieved context [2]. This integration of modules within LangChain facilitates the creation of advanced AI systems.\n",
      "\n",
      "\n",
      "References:\n",
      "[1] Getting Started with LangChain - Tech Blogger (2024)\n",
      "    https://techblog.example/langchain-intro\n",
      "[2] LangChain Documentation - RetrievalQA - LangChain Team (2024)\n",
      "[3] ChromaDB Documentation - Quick Start - ChromaDB Team (2024)\n",
      "[4] Production RAG Best Practices - ML Engineer (2024)\n",
      "    https://mlblog.example/rag-best-practices\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Query only from blog posts and documentation\n",
    "question = \"How do I get started with LangChain?\"\n",
    "\n",
    "result = cited_rag.query(\n",
    "    question,\n",
    "    filter_dict={\"document_type\": {\"$in\": [\"blog\", \"documentation\"]}}\n",
    ")\n",
    "\n",
    "cited_rag.print_result(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7803c4",
   "metadata": {},
   "source": [
    "## Part 5: Multi-Retriever RAG\n",
    "\n",
    "**Advanced Pattern: Multiple Retrievers**\n",
    "- Query multiple collections simultaneously\n",
    "- Combine results from different sources\n",
    "- Useful for diverse knowledge bases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4f57ff",
   "metadata": {},
   "source": [
    "### 5a. Creating Separate Collections\n",
    "\n",
    "Let's simulate having separate vector stores for different document types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1f8b1a2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Created 3 separate collections:\n",
      "   ðŸ“„ Papers: 3 documents\n",
      "   ðŸ“ Blogs: 2 documents\n",
      "   ðŸ“š Documentation: 2 documents\n"
     ]
    }
   ],
   "source": [
    "# Create separate vector stores for different document types\n",
    "papers_docs = [d for d in documents if d.metadata['document_type'] == 'paper']\n",
    "blog_docs = [d for d in documents if d.metadata['document_type'] == 'blog']\n",
    "doc_docs = [d for d in documents if d.metadata['document_type'] == 'documentation']\n",
    "\n",
    "# Create separate vector stores\n",
    "papers_store = Chroma(\n",
    "    collection_name=\"papers\",\n",
    "    embedding_function=embeddings,\n",
    "    persist_directory=\"./chroma_db_day4_papers\"\n",
    ")\n",
    "papers_store.add_documents(papers_docs)\n",
    "\n",
    "blog_store = Chroma(\n",
    "    collection_name=\"blogs\",\n",
    "    embedding_function=embeddings,\n",
    "    persist_directory=\"./chroma_db_day4_blogs\"\n",
    ")\n",
    "blog_store.add_documents(blog_docs)\n",
    "\n",
    "doc_store = Chroma(\n",
    "    collection_name=\"documentation\",\n",
    "    embedding_function=embeddings,\n",
    "    persist_directory=\"./chroma_db_day4_docs\"\n",
    ")\n",
    "doc_store.add_documents(doc_docs)\n",
    "\n",
    "print(f\"âœ… Created 3 separate collections:\")\n",
    "print(f\"   ðŸ“„ Papers: {len(papers_docs)} documents\")\n",
    "print(f\"   ðŸ“ Blogs: {len(blog_docs)} documents\")\n",
    "print(f\"   ðŸ“š Documentation: {len(doc_docs)} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a2f256",
   "metadata": {},
   "source": [
    "### 5b. Ensemble Retriever (Multi-Source)\n",
    "\n",
    "Combine multiple retrievers to search across all sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2ccaf8c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Retrieved 6 documents from multiple sources:\n",
      "\n",
      "1. [PAPER] RAG: Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\n",
      "   Content: Retrieval Augmented Generation (RAG) combines the power of large language models with external knowl...\n",
      "\n",
      "2. [PAPER] Emergent Abilities of Large Language Models\n",
      "   Content: Large Language Models (LLMs) like GPT-4 demonstrate emergent abilities in reasoning, comprehension, ...\n",
      "\n",
      "3. [BLOG] Getting Started with LangChain\n",
      "   Content: LangChain provides a comprehensive framework for building LLM applications. It includes modules for ...\n",
      "\n",
      "4. [DOCUMENTATION] LangChain Documentation - RetrievalQA\n",
      "   Content: LangChain's RetrievalQA chain combines a retriever with a question-answering chain. It retrieves rel...\n",
      "\n",
      "5. [BLOG] Production RAG Best Practices\n",
      "   Content: When building production RAG systems, consider chunking strategies, embedding models, retrieval algo...\n",
      "\n",
      "6. [DOCUMENTATION] ChromaDB Documentation - Quick Start\n",
      "   Content: ChromaDB is an open-source embedding database. It's designed to be simple, fast, and easy to use. In...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_classic.retrievers import EnsembleRetriever\n",
    "\n",
    "# Create individual retrievers\n",
    "papers_retriever = papers_store.as_retriever(search_kwargs={\"k\": 2})\n",
    "blog_retriever = blog_store.as_retriever(search_kwargs={\"k\": 2})\n",
    "doc_retriever = doc_store.as_retriever(search_kwargs={\"k\": 2})\n",
    "\n",
    "# Combine into ensemble retriever\n",
    "ensemble_retriever = EnsembleRetriever(\n",
    "    retrievers=[papers_retriever, blog_retriever, doc_retriever],\n",
    "    weights=[0.4, 0.3, 0.3]  # Give slightly more weight to papers\n",
    ")\n",
    "\n",
    "# Test the ensemble retriever\n",
    "# NOTE: Using invoke() instead of get_relevant_documents() (modern LangChain practice)\n",
    "question = \"What is LangChain and how does it help with RAG?\"\n",
    "results = ensemble_retriever.invoke(question)\n",
    "\n",
    "print(f\"ðŸ” Retrieved {len(results)} documents from multiple sources:\\n\")\n",
    "for i, doc in enumerate(results, 1):\n",
    "    print(f\"{i}. [{doc.metadata['document_type'].upper()}] {doc.metadata['title']}\")\n",
    "    print(f\"   Content: {doc.page_content[:100]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b7cc8f",
   "metadata": {},
   "source": [
    "### 5c. Multi-Source RAG with Citations\n",
    "\n",
    "Combine ensemble retrieval with our citation system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1a801189",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â“ Question: Explain RAG and how to implement it in practice\n",
      "\n",
      "ðŸ“ Answer:\n",
      "Retrieval Augmented Generation (RAG) is a method that combines the capabilities of large language models (LLMs) with external knowledge retrieval to enhance the quality of generated content. By integrating external knowledge retrieval, RAG helps reduce hallucinations and improves the accuracy and relevance of the information provided [1]. This approach is particularly useful for generating more up-to-date and reliable content.\n",
      "\n",
      "To implement RAG in practice, several key considerations need to be taken into account. When building production RAG systems, it is important to consider various factors such as chunking strategies, embedding models, retrieval algorithms, and prompt engineering. Each of these choices plays a crucial role in determining the overall quality and effectiveness of the RAG system [3].\n",
      "\n",
      "One practical way to implement RAG is through the use of LangChain's RetrievalQA chain. This chain combines a retriever with a question-answering chain, allowing relevant documents to be retrieved and passed to an LLM for generating answers based on the retrieved context [4]. Additionally, LangChain offers a comprehensive framework for building LLM applications, providing modules for prompts, chains, agents, memory, and retrieval. This framework simplifies the development of complex AI systems that incorporate RAG capabilities [5].\n",
      "\n",
      "In addition to utilizing LangChain, another useful tool for implementing RAG is ChromaDB, an open-source embedding database designed for simplicity, speed, and ease of use. ChromaDB can be easily installed with the command \"pip install chromadb\" and involves basic usage steps such as creating a client, creating a collection, and adding documents. By incorporating ChromaDB into the implementation of RAG systems, developers can enhance the efficiency and performance of their knowledge retrieval processes [6].\n",
      "\n",
      "Overall, the implementation of RAG involves leveraging the capabilities of LLMs, integrating external knowledge retrieval mechanisms, and making informed choices regarding system design and tool selection to optimize the quality and accuracy of generated content.\n",
      "\n",
      "\n",
      "References:\n",
      "[1] RAG: Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks - Dr. John Doe (2023), p. 3\n",
      "[2] Emergent Abilities of Large Language Models - Dr. Jane Smith (2023), p. 1\n",
      "[3] Production RAG Best Practices - ML Engineer (2024)\n",
      "    https://mlblog.example/rag-best-practices\n",
      "[4] LangChain Documentation - RetrievalQA - LangChain Team (2024)\n",
      "[5] Getting Started with LangChain - Tech Blogger (2024)\n",
      "    https://techblog.example/langchain-intro\n",
      "[6] ChromaDB Documentation - Quick Start - ChromaDB Team (2024)\n",
      "\n",
      "\n",
      "ðŸ“Š Source Breakdown:\n",
      "   paper: 2 document(s)\n",
      "   blog: 2 document(s)\n",
      "   documentation: 2 document(s)\n"
     ]
    }
   ],
   "source": [
    "from langchain_classic.retrievers import EnsembleRetriever\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "class MultiSourceRAG:\n",
    "    \"\"\"RAG system that queries multiple sources and provides citations\"\"\"\n",
    "    \n",
    "    def __init__(self, llm, retrievers_dict, default_k=2):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            llm: Language model\n",
    "            retrievers_dict: Dict of {source_name: (vectorstore, weight)}\n",
    "            default_k: Number of docs to retrieve per source\n",
    "        \"\"\"\n",
    "        self.llm = llm\n",
    "        self.retrievers_dict = retrievers_dict\n",
    "        self.default_k = default_k\n",
    "        \n",
    "        # Create ensemble retriever\n",
    "        retrievers = []\n",
    "        weights = []\n",
    "        for source_name, (vectorstore, weight) in retrievers_dict.items():\n",
    "            retrievers.append(vectorstore.as_retriever(search_kwargs={\"k\": default_k}))\n",
    "            weights.append(weight)\n",
    "        \n",
    "        self.ensemble_retriever = EnsembleRetriever(\n",
    "            retrievers=retrievers,\n",
    "            weights=weights\n",
    "        )\n",
    "        \n",
    "        self.prompt = PromptTemplate(\n",
    "            template=\"\"\"You are a research assistant that provides well-cited answers from multiple sources.\n",
    "\n",
    "Use the context below to answer the question. Each piece of context has a citation number [1], [2], etc.\n",
    "Always cite your sources using these numbers when making claims.\n",
    "\n",
    "Context from multiple sources:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Provide a comprehensive, well-cited answer:\"\"\",\n",
    "            input_variables=[\"context\", \"question\"]\n",
    "        )\n",
    "        \n",
    "        # Create LCEL chain (modern LangChain approach)\n",
    "        self.chain = (\n",
    "            {\"context\": RunnablePassthrough(), \"question\": RunnablePassthrough()}\n",
    "            | self.prompt\n",
    "            | self.llm\n",
    "            | StrOutputParser()\n",
    "        )\n",
    "    \n",
    "    def query(self, question: str):\n",
    "        \"\"\"Query across all sources\"\"\"\n",
    "        \n",
    "        # Retrieve from all sources\n",
    "        retrieved_docs = self.ensemble_retriever.invoke(question)\n",
    "        \n",
    "        # Format with citations\n",
    "        context, citations = format_citations(retrieved_docs)\n",
    "        \n",
    "        # Generate answer\n",
    "        answer = self.chain.invoke({\"context\": context, \"question\": question})\n",
    "        \n",
    "        return {\n",
    "            \"question\": question,\n",
    "            \"answer\": answer,\n",
    "            \"citations\": citations,\n",
    "            \"source_documents\": retrieved_docs,\n",
    "            \"source_breakdown\": self._get_source_breakdown(retrieved_docs)\n",
    "        }\n",
    "    \n",
    "    def _get_source_breakdown(self, docs):\n",
    "        \"\"\"Count documents by source type\"\"\"\n",
    "        breakdown = {}\n",
    "        for doc in docs:\n",
    "            doc_type = doc.metadata.get('document_type', 'unknown')\n",
    "            breakdown[doc_type] = breakdown.get(doc_type, 0) + 1\n",
    "        return breakdown\n",
    "    \n",
    "    def print_result(self, result):\n",
    "        \"\"\"Pretty print with source breakdown\"\"\"\n",
    "        print(\"â“ Question:\", result['question'])\n",
    "        print(\"\\nðŸ“ Answer:\")\n",
    "        print(result['answer'])\n",
    "        print(format_citation_list(result['citations']))\n",
    "        print(\"\\nðŸ“Š Source Breakdown:\")\n",
    "        for source, count in result['source_breakdown'].items():\n",
    "            print(f\"   {source}: {count} document(s)\")\n",
    "\n",
    "# Create multi-source RAG\n",
    "retrievers_config = {\n",
    "    \"papers\": (papers_store, 0.4),\n",
    "    \"blogs\": (blog_store, 0.3),\n",
    "    \"docs\": (doc_store, 0.3)\n",
    "}\n",
    "\n",
    "multi_rag = MultiSourceRAG(llm, retrievers_config, default_k=2)\n",
    "\n",
    "# Test it\n",
    "question = \"Explain RAG and how to implement it in practice\"\n",
    "result = multi_rag.query(question)\n",
    "multi_rag.print_result(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ff58bd",
   "metadata": {},
   "source": [
    "## Part 6: Reranking for Better Retrieval\n",
    "\n",
    "**Why Reranking?**\n",
    "- Initial retrieval (embedding similarity) isn't perfect\n",
    "- Reranking models better understand query-document relevance\n",
    "- Significantly improves RAG accuracy\n",
    "- Popular rerankers: Cohere, Sentence Transformers, cross-encoders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e8231e",
   "metadata": {},
   "source": [
    "### 6a. Simple Score-Based Reranking\n",
    "\n",
    "Implement a basic reranker using relevance scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4ca4ce41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‹ Initial Retrieval (top 6):\n",
      "1. [paper] Vector Databases for AI Applications\n",
      "2. [documentation] ChromaDB Documentation - Quick Start\n",
      "3. [blog] Production RAG Best Practices\n",
      "4. [paper] RAG: Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\n",
      "5. [documentation] LangChain Documentation - RetrievalQA\n",
      "6. [blog] Getting Started with LangChain\n",
      "\n",
      "======================================================================\n",
      "\n",
      "â­ After Reranking (top 4):\n",
      "1. [paper] Vector Databases for AI Applications\n",
      "   Year: 2024\n",
      "2. [documentation] ChromaDB Documentation - Quick Start\n",
      "   Year: 2024\n",
      "3. [paper] RAG: Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\n",
      "   Year: 2023\n",
      "4. [blog] Production RAG Best Practices\n",
      "   Year: 2024\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.documents import Document\n",
    "from typing import List, Tuple\n",
    "\n",
    "class SimpleReranker:\n",
    "    \"\"\"Basic reranker using similarity scores and metadata\"\"\"\n",
    "    \n",
    "    def __init__(self, vectorstore):\n",
    "        self.vectorstore = vectorstore\n",
    "    \n",
    "    def rerank(self, query: str, documents: List[Document], top_k: int = 4) -> List[Document]:\n",
    "        \"\"\"\n",
    "        Rerank documents based on:\n",
    "        1. Similarity score\n",
    "        2. Document type preference (papers > blogs > news)\n",
    "        3. Recency (newer = better)\n",
    "        \"\"\"\n",
    "        # Get similarity scores\n",
    "        docs_with_scores = self.vectorstore.similarity_search_with_score(query, k=len(documents)*2)\n",
    "        \n",
    "        # Create score map\n",
    "        doc_scores = {}\n",
    "        for doc, score in docs_with_scores:\n",
    "            # Lower score = more similar in some embeddings\n",
    "            doc_scores[doc.page_content] = score\n",
    "        \n",
    "        # Rerank with custom scoring\n",
    "        ranked_docs = []\n",
    "        for doc in documents:\n",
    "            # Base score from similarity\n",
    "            base_score = doc_scores.get(doc.page_content, 1.0)\n",
    "            \n",
    "            # Boost based on document type\n",
    "            type_boost = {\n",
    "                'paper': 0.1,\n",
    "                'documentation': 0.05,\n",
    "                'blog': 0.02,\n",
    "                'news': 0.0\n",
    "            }.get(doc.metadata.get('document_type', ''), 0)\n",
    "            \n",
    "            # Boost based on recency\n",
    "            year = doc.metadata.get('year', 2020)\n",
    "            recency_boost = (year - 2020) * 0.01\n",
    "            \n",
    "            # Calculate final score (lower is better)\n",
    "            final_score = base_score - type_boost - recency_boost\n",
    "            \n",
    "            ranked_docs.append((doc, final_score))\n",
    "        \n",
    "        # Sort by score and return top_k\n",
    "        ranked_docs.sort(key=lambda x: x[1])\n",
    "        return [doc for doc, score in ranked_docs[:top_k]]\n",
    "\n",
    "# Test reranking\n",
    "reranker = SimpleReranker(vectorstore)\n",
    "\n",
    "question = \"How do I use vector databases?\"\n",
    "\n",
    "# Get initial results\n",
    "initial_docs = vectorstore.similarity_search(question, k=6)\n",
    "print(\"ðŸ“‹ Initial Retrieval (top 6):\")\n",
    "for i, doc in enumerate(initial_docs, 1):\n",
    "    print(f\"{i}. [{doc.metadata['document_type']}] {doc.metadata['title']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70 + \"\\n\")\n",
    "\n",
    "# Rerank to get top 4\n",
    "reranked_docs = reranker.rerank(question, initial_docs, top_k=4)\n",
    "print(\"â­ After Reranking (top 4):\")\n",
    "for i, doc in enumerate(reranked_docs, 1):\n",
    "    print(f\"{i}. [{doc.metadata['document_type']}] {doc.metadata['title']}\")\n",
    "    print(f\"   Year: {doc.metadata['year']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1b2bf8",
   "metadata": {},
   "source": [
    "### 6b. Contextual Compression\n",
    "\n",
    "Use LangChain's built-in compression retriever to filter and compress retrieved content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50696aae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Querying with contextual compression...\n",
      "\n",
      "Retrieved 2 compressed documents:\n",
      "\n",
      "1. Production RAG Best Practices\n",
      "   Compressed content: consider chunking strategies, embedding models, retrieval algorithms, and prompt engineering.\n",
      "\n",
      "2. RAG: Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\n",
      "   Compressed content: Retrieval Augmented Generation (RAG) combines the power of large language models with external knowledge retrieval. This approach significantly reduces hallucinations and provides more accurate, up-to-date information.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_classic.retrievers import ContextualCompressionRetriever\n",
    "from langchain_classic.retrievers.document_compressors import LLMChainExtractor\n",
    "\n",
    "# Create a compressor that extracts only relevant parts\n",
    "compressor = LLMChainExtractor.from_llm(llm)\n",
    "\n",
    "# Wrap base retriever with compression\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor,\n",
    "    base_retriever=vectorstore.as_retriever(search_kwargs={\"k\": 4})\n",
    ")\n",
    "\n",
    "# Test compression\n",
    "question = \"What are the main benefits of RAG systems?\"\n",
    "\n",
    "print(\"ðŸ” Querying with contextual compression...\\n\")\n",
    "compressed_docs = compression_retriever.invoke(question)\n",
    "\n",
    "print(f\"Retrieved {len(compressed_docs)} compressed documents:\\n\")\n",
    "for i, doc in enumerate(compressed_docs, 1):\n",
    "    print(f\"{i}. {doc.metadata['title']}\")\n",
    "    print(f\"   Compressed content: {doc.page_content}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279da8aa",
   "metadata": {},
   "source": [
    "## Part 7: Retrieval Quality Metrics\n",
    "\n",
    "**Measuring Success:**\n",
    "- Precision: Are retrieved docs relevant?\n",
    "- Recall: Did we retrieve all relevant docs?\n",
    "- MRR (Mean Reciprocal Rank): Position of first relevant doc\n",
    "- NDCG: Ranking quality\n",
    "\n",
    "Let's implement basic evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b03457c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Retrieval Quality Metrics\n",
      "==================================================\n",
      "Query: What is RAG?\n",
      "\n",
      "Retrieved documents:\n",
      "  1. [âœ“] RAG: Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\n",
      "  2. [âœ“] Production RAG Best Practices\n",
      "  3. [âœ“] LangChain Documentation - RetrievalQA\n",
      "  4. [âœ—] Getting Started with LangChain\n",
      "  5. [âœ—] Emergent Abilities of Large Language Models\n",
      "\n",
      "Metrics:\n",
      "  Precision@5: 60.00% (What fraction of retrieved docs are relevant)\n",
      "  Recall@5: 100.00% (What fraction of relevant docs were retrieved)\n",
      "  MRR: 1.000 (Reciprocal rank of first relevant doc)\n",
      "  F1 Score: 75.00% (Harmonic mean of precision and recall)\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Set\n",
    "\n",
    "class RetrievalEvaluator:\n",
    "    \"\"\"Evaluate retrieval quality\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def precision_at_k(retrieved_docs: List[str], relevant_docs: Set[str], k: int) -> float:\n",
    "        \"\"\"\n",
    "        Precision@K: What fraction of top-k retrieved docs are relevant?\n",
    "        \"\"\"\n",
    "        top_k = retrieved_docs[:k]\n",
    "        relevant_in_top_k = len([doc for doc in top_k if doc in relevant_docs])\n",
    "        return relevant_in_top_k / k if k > 0 else 0.0\n",
    "    \n",
    "    @staticmethod\n",
    "    def recall_at_k(retrieved_docs: List[str], relevant_docs: Set[str], k: int) -> float:\n",
    "        \"\"\"\n",
    "        Recall@K: What fraction of all relevant docs are in top-k?\n",
    "        \"\"\"\n",
    "        top_k = retrieved_docs[:k]\n",
    "        relevant_in_top_k = len([doc for doc in top_k if doc in relevant_docs])\n",
    "        return relevant_in_top_k / len(relevant_docs) if len(relevant_docs) > 0 else 0.0\n",
    "    \n",
    "    @staticmethod\n",
    "    def mean_reciprocal_rank(retrieved_docs: List[str], relevant_docs: Set[str]) -> float:\n",
    "        \"\"\"\n",
    "        MRR: 1 / (position of first relevant doc)\n",
    "        Higher is better. 1.0 means first doc was relevant.\n",
    "        \"\"\"\n",
    "        for i, doc in enumerate(retrieved_docs, 1):\n",
    "            if doc in relevant_docs:\n",
    "                return 1.0 / i\n",
    "        return 0.0\n",
    "\n",
    "# Example evaluation\n",
    "question = \"What is RAG?\"\n",
    "\n",
    "# Retrieve documents\n",
    "retrieved = vectorstore.similarity_search(question, k=5)\n",
    "retrieved_titles = [doc.metadata['title'] for doc in retrieved]\n",
    "\n",
    "# Define ground truth (which documents SHOULD be retrieved for this query)\n",
    "relevant_titles = {\n",
    "    \"RAG: Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\",\n",
    "    \"Production RAG Best Practices\",\n",
    "    \"LangChain Documentation - RetrievalQA\"\n",
    "}\n",
    "\n",
    "# Evaluate\n",
    "evaluator = RetrievalEvaluator()\n",
    "\n",
    "precision = evaluator.precision_at_k(retrieved_titles, relevant_titles, k=5)\n",
    "recall = evaluator.recall_at_k(retrieved_titles, relevant_titles, k=5)\n",
    "mrr = evaluator.mean_reciprocal_rank(retrieved_titles, relevant_titles)\n",
    "\n",
    "print(\"ðŸ“Š Retrieval Quality Metrics\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Query: {question}\")\n",
    "print(f\"\\nRetrieved documents:\")\n",
    "for i, title in enumerate(retrieved_titles, 1):\n",
    "    relevant_marker = \"âœ“\" if title in relevant_titles else \"âœ—\"\n",
    "    print(f\"  {i}. [{relevant_marker}] {title}\")\n",
    "\n",
    "print(f\"\\nMetrics:\")\n",
    "print(f\"  Precision@5: {precision:.2%} (What fraction of retrieved docs are relevant)\")\n",
    "print(f\"  Recall@5: {recall:.2%} (What fraction of relevant docs were retrieved)\")\n",
    "print(f\"  MRR: {mrr:.3f} (Reciprocal rank of first relevant doc)\")\n",
    "\n",
    "# F1 Score\n",
    "f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "print(f\"  F1 Score: {f1:.2%} (Harmonic mean of precision and recall)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9169adf8",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Part 8: Exercises & Challenges\n",
    "\n",
    "Now it's your turn! Try these exercises to reinforce your learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46e966f",
   "metadata": {},
   "source": [
    "### Exercise 1: Build a Multi-Author Knowledge Base\n",
    "\n",
    "**Task**: Create a document collection with at least 10 documents from 3 different authors on a topic of your choice (e.g., machine learning, web development, finance).\n",
    "\n",
    "**Requirements**:\n",
    "1. Include rich metadata (author, year, category, document type)\n",
    "2. Create a function that retrieves documents by author\n",
    "3. Build a RAG system that can answer questions and cite specific authors\n",
    "\n",
    "**Bonus**: Add a feature that shows how many times each author was cited in a response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb866d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code for Exercise 1 here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f65635e",
   "metadata": {},
   "source": [
    "### Exercise 2: Implement a Smart Document Router\n",
    "\n",
    "**Task**: Build a system that automatically routes questions to the most appropriate document collection based on query content.\n",
    "\n",
    "**Requirements**:\n",
    "1. Create 3 separate vector stores for different topics (e.g., \"python\", \"databases\", \"ai\")\n",
    "2. Implement a classifier that determines which store(s) to query\n",
    "3. Route the query appropriately and return cited results\n",
    "\n",
    "**Hint**: You can use the LLM itself to classify the query, or use keyword matching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2c08f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code for Exercise 2 here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee23503",
   "metadata": {},
   "source": [
    "### Exercise 3: Citation Format Customization\n",
    "\n",
    "**Task**: Extend the citation system to support different citation formats.\n",
    "\n",
    "**Requirements**:\n",
    "1. Implement at least 2 citation formats:\n",
    "   - APA: Author, A. (Year). Title. Source.\n",
    "   - MLA: Author. \"Title.\" Source, Year.\n",
    "2. Allow users to choose their preferred format\n",
    "3. Update the CitedRAG class to use the selected format\n",
    "\n",
    "**Bonus**: Add a \"footnote\" style where citations appear as superscript numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc23f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code for Exercise 3 here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1785844b",
   "metadata": {},
   "source": [
    "### Exercise 4: Advanced Reranking\n",
    "\n",
    "**Task**: Implement a more sophisticated reranker that considers query-document semantic similarity.\n",
    "\n",
    "**Requirements**:\n",
    "1. Use cross-encoder style scoring (you can simulate with LLM if needed)\n",
    "2. For each retrieved document, ask the LLM to rate relevance 0-10\n",
    "3. Rerank based on these scores\n",
    "4. Compare results with baseline retrieval\n",
    "\n",
    "**Bonus**: Implement batch processing to make this efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c507ca01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code for Exercise 4 here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48c38b3",
   "metadata": {},
   "source": [
    "### Exercise 5: Build a Research Assistant\n",
    "\n",
    "**Task**: Create a comprehensive research assistant that combines all concepts from today.\n",
    "\n",
    "**Requirements**:\n",
    "1. Multi-source document collection (papers, blogs, docs)\n",
    "2. Support for metadata filtering\n",
    "3. Citation system with multiple formats\n",
    "4. Reranking for better results\n",
    "5. Quality metrics tracking\n",
    "\n",
    "**Features to implement**:\n",
    "- `search(query, filters)`: Search with optional filters\n",
    "- `ask(question, sources, citation_format)`: Ask questions with source preferences\n",
    "- `get_stats()`: Show retrieval quality statistics\n",
    "- `export_citations(format)`: Export used citations\n",
    "\n",
    "**Bonus**: Add a conversation mode that maintains context and builds a bibliography of all cited sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57931543",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code for Exercise 5 here\n",
    "\n",
    "class ResearchAssistant:\n",
    "    \"\"\"\n",
    "    A comprehensive research assistant with:\n",
    "    - Multi-source RAG\n",
    "    - Citation management\n",
    "    - Reranking\n",
    "    - Quality metrics\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, llm, vectorstores_dict):\n",
    "        # Initialize your research assistant\n",
    "        pass\n",
    "    \n",
    "    def search(self, query, filters=None):\n",
    "        \"\"\"Search across sources with optional filters\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def ask(self, question, sources=None, citation_format=\"numbered\"):\n",
    "        \"\"\"Ask a question and get a cited answer\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def get_stats(self):\n",
    "        \"\"\"Get retrieval quality statistics\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def export_citations(self, format=\"txt\"):\n",
    "        \"\"\"Export all citations used\"\"\"\n",
    "        pass\n",
    "\n",
    "# Test your research assistant\n",
    "# assistant = ResearchAssistant(llm, {...})\n",
    "# result = assistant.ask(\"What is RAG?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f445261",
   "metadata": {},
   "source": [
    "## ðŸŽ“ Summary & Key Takeaways\n",
    "\n",
    "Congratulations! You've learned advanced RAG techniques. Here's what we covered:\n",
    "\n",
    "### Key Concepts\n",
    "1. **Multi-Source RAG**: Query multiple document collections simultaneously\n",
    "2. **Metadata Filtering**: Use document metadata for targeted retrieval\n",
    "3. **Citation Systems**: Track and attribute sources properly\n",
    "4. **Reranking**: Improve retrieval accuracy beyond simple similarity\n",
    "5. **Quality Metrics**: Measure and evaluate retrieval performance\n",
    "\n",
    "### Advanced Patterns\n",
    "- **Ensemble Retrievers**: Combine multiple retrievers with weighted scoring\n",
    "- **Contextual Compression**: Extract only relevant parts of documents\n",
    "- **Source Attribution**: Track which documents contributed to answers\n",
    "- **Custom Rerankers**: Implement domain-specific ranking logic\n",
    "\n",
    "### Production Considerations\n",
    "- **Transparency**: Always provide sources for fact-checking\n",
    "- **Flexibility**: Support different citation formats for different audiences\n",
    "- **Quality**: Use metrics to continuously improve retrieval\n",
    "- **Efficiency**: Balance accuracy with performance (reranking is expensive)\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸš€ Next Steps\n",
    "\n",
    "Tomorrow (Day 5), we'll explore:\n",
    "- **ReAct Agents**: Combine reasoning and actions\n",
    "- **Tool Integration**: Give LLMs access to external tools\n",
    "- **Function Calling**: Structured outputs and API calls\n",
    "- **Agent Loops**: Iterative problem-solving\n",
    "\n",
    "### Preparation for Day 5\n",
    "- Review the agent concepts from LangChain docs\n",
    "- Think about what tools an AI assistant might need\n",
    "- Consider problems that require multiple steps to solve\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“š Additional Resources\n",
    "\n",
    "**Reading:**\n",
    "- [LangChain Retrieval Docs](https://python.langchain.com/docs/modules/data_connection/)\n",
    "- [RAG Paper (Lewis et al.)](https://arxiv.org/abs/2005.11401)\n",
    "- [Dense Passage Retrieval](https://arxiv.org/abs/2004.04906)\n",
    "\n",
    "**Advanced Topics:**\n",
    "- Hybrid search (combining dense and sparse retrieval)\n",
    "- Query expansion and rewriting\n",
    "- Multi-hop reasoning over retrieved documents\n",
    "- Adaptive retrieval (deciding when to retrieve more)\n",
    "\n",
    "**Tools to Explore:**\n",
    "- Cohere Rerank API\n",
    "- Sentence Transformers for reranking\n",
    "- LlamaIndex for advanced indexing strategies\n",
    "\n",
    "---\n",
    "\n",
    "Great work today! ðŸŽ‰"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
