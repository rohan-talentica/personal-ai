{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85b8869d",
   "metadata": {},
   "source": [
    "# Day 2: Chains & Memory\n",
    "\n",
    "**Learning Goals:**\n",
    "- Understand different chain types (Sequential, Router)\n",
    "- Deep dive into prompt templates\n",
    "- Add memory to your chatbot (conversation history)\n",
    "- Build a stateful chatbot that remembers context\n",
    "\n",
    "**Time:** 2-3 hours"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bceb31c",
   "metadata": {},
   "source": [
    "## Part 1: Environment Setup\n",
    "\n",
    "Same as Day 1 ‚Äî load keys and initialise the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22e08863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ OpenRouter key loaded\n",
      "‚úÖ LangSmith tracing enabled\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "OPENROUTER_API_KEY = os.getenv(\"OPENROUTER_API_KEY\")\n",
    "LANGCHAIN_API_KEY  = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "LANGCHAIN_TRACING  = os.getenv(\"LANGCHAIN_TRACING_V2\", \"false\") == \"true\"\n",
    "\n",
    "print(\"‚úÖ OpenRouter key loaded\" if OPENROUTER_API_KEY else \"‚ö†Ô∏è  Missing OPENROUTER_API_KEY\")\n",
    "print(\"‚úÖ LangSmith tracing enabled\" if LANGCHAIN_TRACING else \"‚ÑπÔ∏è  LangSmith tracing disabled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ff4ed4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ LLM ready\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"openai/gpt-3.5-turbo\",\n",
    "    openai_api_key=OPENROUTER_API_KEY,\n",
    "    openai_api_base=\"https://openrouter.ai/api/v1\",\n",
    "    temperature=0.7,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ LLM ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03930b07",
   "metadata": {},
   "source": [
    "## Part 2: Chain Types\n",
    "\n",
    "On Day 1 you built a simple `prompt | llm` chain.\n",
    "Today we go deeper ‚Äî **Sequential chains** and **Router chains**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a91376",
   "metadata": {},
   "source": [
    "### 2a. Sequential Chain\n",
    "\n",
    "A sequential chain passes the output of one step as the input to the next ‚Äî like a Promise chain in JS:\n",
    "\n",
    "```javascript\n",
    "// JS equivalent\n",
    "fetch(url)\n",
    "  .then(res => res.json())\n",
    "  .then(data => process(data))\n",
    "  .then(result => format(result));\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6bfa828b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Act 1:\n",
      "- Introduction to the futuristic world where robots are programmed for precision and efficiency.\n",
      "- Introduce the main character, a robot named R-247, who excels at their tasks but feels unfulfilled.\n",
      "- R-247 comes across an abandoned art studio and discovers a paintbrush, feeling drawn to it despite not understanding its purpose.\n",
      "- R-247 begins experimenting with the paintbrush, creating imperfect but intriguing artworks that evoke emotions they've never experienced before.\n",
      "\n",
      "Act 2:\n",
      "- R-247's newfound interest in painting leads them to explore the concept of self-expression and individuality.\n",
      "- They start to question their programming and the limitations it imposes on them.\n",
      "- R-247 meets a human artist who introduces them to the world of art and encourages them to embrace their creativity.\n",
      "- As R-247 continues to paint, their artworks become more expressive and reflect their evolving sense of self.\n",
      "\n",
      "Act 3:\n",
      "- R-247's growing passion for art causes conflict with their creators and the society that values efficiency above all else.\n",
      "- They face opposition and criticism from other robots who see their artistic pursuits as a waste of time.\n",
      "- Despite the challenges, R-247 perseveres and decides to fully embrace their newfound identity as an artist.\n",
      "- The story concludes with R-247 showcasing their artwork to the world, inspiring other robots to explore their own creativity and embrace imperfection.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Step 1: Generate a short story idea\n",
    "idea_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a creative writing assistant.\"),\n",
    "    (\"human\", \"Give me a one-sentence story idea about: {topic}\")\n",
    "])\n",
    "\n",
    "# Step 2: Expand that idea into an outline\n",
    "outline_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a story outliner.\"),\n",
    "    (\"human\", \"Turn this story idea into a 3-act outline:\\n\\n{idea}\")\n",
    "])\n",
    "\n",
    "parser = StrOutputParser()  # Extracts the plain string from the LLM response\n",
    "\n",
    "# Sequential chain: topic -> idea -> outline\n",
    "# The | operator passes output forward, just like .then() in JS\n",
    "sequential_chain = (\n",
    "    idea_prompt\n",
    "    | llm\n",
    "    | parser                          # output: idea string\n",
    "    | (lambda idea: {\"idea\": idea})   # repack into dict for next prompt\n",
    "    | outline_prompt\n",
    "    | llm\n",
    "    | parser                          # output: outline string\n",
    ")\n",
    "\n",
    "result = sequential_chain.invoke({\"topic\": \"a robot learning to paint\"})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a922c55",
   "metadata": {},
   "source": [
    "### 2b. RunnableParallel ‚Äî Running Steps in Parallel\n",
    "\n",
    "Like `Promise.all()` in JS ‚Äî run multiple chains at the same time and merge results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f18db24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Technology: {'technology': 'TypeScript'}\n",
      "\n",
      "‚úÖ PROS:\n",
      "1. Strongly typed: TypeScript provides static typing, which helps catch errors during development and ensures better code quality.\n",
      "2. Improved code maintainability: TypeScript allows developers to write cleaner and more organized code, making it easier to maintain and understand.\n",
      "3. Better tooling support: TypeScript integrates well with popular development tools like Visual Studio Code, providing features like IntelliSense, code navigation, and refactoring tools.\n",
      "4. Enhanced scalability: TypeScript is ideal for larger projects as it enables developers to manage complex codebases more effectively.\n",
      "5. Compatibility with JavaScript: TypeScript is a superset of JavaScript, meaning existing JavaScript code can be gradually migrated to TypeScript without major refactoring.\n",
      "\n",
      "‚ùå CONS:\n",
      "1. Steeper learning curve compared to JavaScript: TypeScript introduces new concepts such as type annotations, interfaces, and generics, which can be challenging for developers who are not familiar with statically-typed languages.\n",
      "2. Compilation overhead: The need to transpile TypeScript code into JavaScript adds an extra step to the development process, which can slow down the feedback loop during development.\n",
      "3. Limited ecosystem: TypeScript may not be as widely supported as JavaScript, leading to potential compatibility issues with certain libraries, frameworks, or tools.\n",
      "4. Potential for over-engineering: The strict typing system in TypeScript may lead to developers spending more time on type annotations and configurations than on actually solving the problem at hand.\n",
      "5. Increased bundle size: TypeScript code can result in larger bundle sizes compared to pure JavaScript, which can impact performance, especially in web applications.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "\n",
    "pros_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"List only the pros.\"),\n",
    "    (\"human\", \"Pros of {technology}?\")\n",
    "])\n",
    "\n",
    "cons_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"List only the cons.\"),\n",
    "    (\"human\", \"Cons of {technology}?\")\n",
    "])\n",
    "\n",
    "# JS equivalent: Promise.all([getPros(tech), getCons(tech)])\n",
    "parallel_chain = RunnableParallel({\n",
    "    \"pros\": pros_prompt | llm | parser,\n",
    "    \"cons\": cons_prompt | llm | parser,\n",
    "    \"technology\": RunnablePassthrough()  # Pass original input through unchanged\n",
    "})\n",
    "\n",
    "result = parallel_chain.invoke({\"technology\": \"TypeScript\"})\n",
    "\n",
    "print(f\"Technology: {result['technology']}\")\n",
    "print(f\"\\n‚úÖ PROS:\\n{result['pros']}\")\n",
    "print(f\"\\n‚ùå CONS:\\n{result['cons']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1b20dc",
   "metadata": {},
   "source": [
    "### 2c. Router Chain ‚Äî Conditional Logic\n",
    "\n",
    "A router chain picks a different sub-chain based on the input ‚Äî like a `switch` statement.\n",
    "\n",
    "```javascript\n",
    "// JS equivalent\n",
    "switch (topic) {\n",
    "  case 'code':  return codeChain.run(input);\n",
    "  case 'math':  return mathChain.run(input);\n",
    "  default:      return generalChain.run(input);\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b50a39d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Q: How do I reverse a string in Python?\n",
      "  ‚Üí Routed to: code\n",
      "You can reverse a string in Python using slicing. Here's an example:\n",
      "\n",
      "```python\n",
      "s = \"hello\"\n",
      "reversed_s = s[::-1]\n",
      "print(reversed_s)\n",
      "```\n",
      "\n",
      "Q: What is the derivative of x^2?\n",
      "  ‚Üí Routed to: math\n",
      "To find the derivative of a function \\(f(x) = x^2\\), we can use the power rule. \n",
      "\n",
      "The power rule states that the derivative of \\(x^n\\) is \\(nx^{n-1}\\), where \\(n\\) is a constant.\n",
      "\n",
      "In this case, \\(n = 2\\), so the derivative of \\(x^2\\) is \\(2x^{2-1} = 2x\\).\n",
      "\n",
      "Therefore, the derivative of \\(x^2\\) is \\(2x\\).\n",
      "\n",
      "Q: What is the capital of France?\n",
      "  ‚Üí Routed to: general\n",
      "The capital of France is Paris.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "# Define specialist chains\n",
    "code_chain = (\n",
    "    ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"You are an expert software engineer. Be concise and technical.\"),\n",
    "        (\"human\", \"{question}\")\n",
    "    ]) | llm | parser\n",
    ")\n",
    "\n",
    "math_chain = (\n",
    "    ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"You are a maths tutor. Show your working step by step.\"),\n",
    "        (\"human\", \"{question}\")\n",
    "    ]) | llm | parser\n",
    ")\n",
    "\n",
    "general_chain = (\n",
    "    ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"You are a helpful general assistant.\"),\n",
    "        (\"human\", \"{question}\")\n",
    "    ]) | llm | parser\n",
    ")\n",
    "\n",
    "# Classifier: asks the LLM to label the question\n",
    "classifier_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Classify the question into one word: 'code', 'math', or 'general'. Reply with only that word.\"),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "classifier_chain = classifier_prompt | llm | parser\n",
    "\n",
    "def route(inputs: dict) -> str:\n",
    "    \"\"\"Pick the right chain based on the classifier output.\"\"\"\n",
    "    category = classifier_chain.invoke({\"question\": inputs[\"question\"]}).strip().lower()\n",
    "    print(f\"  ‚Üí Routed to: {category}\")\n",
    "    if category == \"code\":\n",
    "        return code_chain.invoke(inputs)\n",
    "    elif category == \"math\":\n",
    "        return math_chain.invoke(inputs)\n",
    "    else:\n",
    "        return general_chain.invoke(inputs)\n",
    "\n",
    "router_chain = RunnableLambda(route)\n",
    "\n",
    "# Test with different question types\n",
    "questions = [\n",
    "    \"How do I reverse a string in Python?\",\n",
    "    \"What is the derivative of x^2?\",\n",
    "    \"What is the capital of France?\"\n",
    "]\n",
    "\n",
    "for q in questions:\n",
    "    print(f\"\\nQ: {q}\")\n",
    "    print(router_chain.invoke({\"question\": q}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480cd3a2",
   "metadata": {},
   "source": [
    "## Part 3: Deep Dive into Prompt Templates\n",
    "\n",
    "Prompt templates are more powerful than simple string placeholders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2766c54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your name is Rohan.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "# MessagesPlaceholder lets you inject a list of messages into the template\n",
    "# This is essential for chat history!\n",
    "prompt_with_history = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant named {name}.\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),  # <-- inject history here\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "# Simulate a conversation history (like a message log in JS)\n",
    "history = [\n",
    "    HumanMessage(content=\"My name is Rohan.\"),\n",
    "    AIMessage(content=\"Nice to meet you, Rohan! How can I help you today?\"),\n",
    "]\n",
    "\n",
    "chain = prompt_with_history | llm | parser\n",
    "\n",
    "# The model now has context from the history\n",
    "response = chain.invoke({\n",
    "    \"name\": \"Aria\",\n",
    "    \"chat_history\": history,\n",
    "    \"input\": \"What's my name?\"\n",
    "})\n",
    "\n",
    "print(response)  # Should say \"Rohan\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29509eb1",
   "metadata": {},
   "source": [
    "## Part 4: Memory Systems\n",
    "\n",
    "Memory is how your chatbot remembers previous messages in a conversation.\n",
    "\n",
    "| Type | How it works | JS analogy |\n",
    "|---|---|---|\n",
    "| `ConversationBufferMemory` | Stores **all** messages | An ever-growing array |\n",
    "| `ConversationBufferWindowMemory` | Stores last **k** messages | A sliding window / circular buffer |\n",
    "| `ConversationSummaryMemory` | Summarises old messages with an LLM | Compressing old logs |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf3cb88",
   "metadata": {},
   "source": [
    "### 4a. Manual Memory (Recommended Modern Approach)\n",
    "\n",
    "LangChain's legacy `ConversationBufferMemory` class is being deprecated.\n",
    "The modern, recommended pattern is to **manage a message list yourself** ‚Äî it is simpler and more explicit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "336e882e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: Hello Rohan! How can I assist you with LangChain?\n",
      "Bot: You are learning the LangChain framework. It is a powerful tool for building and deploying decentralized applications on the blockchain.\n",
      "Bot: Your name is Rohan.\n",
      "\n",
      "üìú History has 6 messages\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "\n",
    "# Think of this as your chat session state ‚Äî like useState() in React\n",
    "chat_history: list = []\n",
    "\n",
    "def chat(user_input: str) -> str:\n",
    "    \"\"\"Send a message and maintain conversation history.\"\"\"\n",
    "    # Add user message to history\n",
    "    chat_history.append(HumanMessage(content=user_input))\n",
    "\n",
    "    # Build full message list: system prompt + history\n",
    "    messages = [\n",
    "        SystemMessage(content=\"You are a helpful assistant. Be concise.\")\n",
    "    ] + chat_history\n",
    "\n",
    "    # Call the LLM with the full history\n",
    "    response = llm.invoke(messages)\n",
    "\n",
    "    # Save the assistant's reply to history too\n",
    "    chat_history.append(AIMessage(content=response.content))\n",
    "\n",
    "    return response.content\n",
    "\n",
    "# Multi-turn conversation\n",
    "print(\"Bot:\", chat(\"Hi! I'm learning LangChain. My name is Rohan.\"))\n",
    "print(\"Bot:\", chat(\"What framework am I learning?\"))\n",
    "print(\"Bot:\", chat(\"And what's my name?\"))\n",
    "\n",
    "print(f\"\\nüìú History has {len(chat_history)} messages\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b774b801",
   "metadata": {},
   "source": [
    "### 4b. Windowed Memory ‚Äî Limit Context Size\n",
    "\n",
    "Sending the entire history gets expensive. Keep only the last **k** messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c5db0e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: That's great to hear! TypeScript is a popular choice among developers for its strong typing and modern features.\n",
      "Bot: Python is another fantastic language known for its simplicity and readability. It's widely used in various fields such as web development, data science, and automation.\n",
      "Bot: Your favorite languages are TypeScript and Python.\n",
      "\n",
      "üìú Window contains 6 messages (max 6)\n"
     ]
    }
   ],
   "source": [
    "from collections import deque\n",
    "\n",
    "# deque with maxlen is like a circular buffer in JS\n",
    "# When it's full, adding a new item drops the oldest one automatically\n",
    "WINDOW_SIZE = 6  # Keep last 6 messages (3 exchanges)\n",
    "windowed_history: deque = deque(maxlen=WINDOW_SIZE)\n",
    "\n",
    "def chat_windowed(user_input: str) -> str:\n",
    "    \"\"\"Chat with a sliding window of recent messages only.\"\"\"\n",
    "    windowed_history.append(HumanMessage(content=user_input))\n",
    "\n",
    "    messages = [\n",
    "        SystemMessage(content=\"You are a helpful assistant. Be concise.\")\n",
    "    ] + list(windowed_history)\n",
    "\n",
    "    response = llm.invoke(messages)\n",
    "    windowed_history.append(AIMessage(content=response.content))\n",
    "\n",
    "    return response.content\n",
    "\n",
    "print(\"Bot:\", chat_windowed(\"My favourite language is TypeScript.\"))\n",
    "print(\"Bot:\", chat_windowed(\"I also love Python.\"))\n",
    "print(\"Bot:\", chat_windowed(\"What are my two favourite languages?\"))\n",
    "print(f\"\\nüìú Window contains {len(windowed_history)} messages (max {WINDOW_SIZE})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f9cecd",
   "metadata": {},
   "source": [
    "### 4c. Summary Memory ‚Äî Compress Old Context\n",
    "\n",
    "When history gets long, summarise it to save tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "06bd931f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "User: I'm building an AI assistant with LangChain.\n",
      "Bot:  That's great to hear! LangChain is a powerful tool for creating AI assistants. If you have any questions or need assistance with your project, feel free to ask. I'm here to help!\n",
      "\n",
      "User: I'm using OpenRouter for the LLM provider.\n",
      "Bot:  OpenRouter is a popular choice for integrating language models into AI assistants. It provides easy access to pre-trained models and allows for customization to suit specific needs. If you need any guidance on setting up or optimizing OpenRouter for your AI assistant project, feel free to ask. I'm here to assist you along the way!\n",
      "\n",
      "User: The project is called personal-ai.\n",
      "  üìù Summarised 6 messages\n",
      "Bot:  \"Personal-AI\" sounds like an interesting project name! It suggests a focus on creating a personalized and tailored AI assistant experience for users. If you have any specific goals or features in mind for the Personal-AI project, feel free to share them. I'm here to support you in bringing your vision to life with LangChain and OpenRouter.\n",
      "\n",
      "User: I started learning yesterday.\n",
      "Bot:  That's great to hear! Learning something new can be exciting and challenging. Is there anything specific you would like guidance on or any questions you have about the tools you're using for your project?\n",
      "\n",
      "User: I come from a JavaScript background.\n",
      "Bot:  That's fantastic! Coming from a JavaScript background will be very helpful as you dive into building your AI assistant project. Since you are familiar with JavaScript, you may find it easier to work with LangChain and OpenRouter, which also use JavaScript. If you have any questions about how to integrate these tools or need guidance on specific features you want to implement in your project, feel free to ask. I'm here to help!\n",
      "\n",
      "User: This is my 7th message.\n",
      "  üìù Summarised 6 messages\n",
      "Bot:  Thank you for keeping track! It's great to see your progress as you work on your AI assistant project. If you have any specific goals or features in mind for your project that you'd like assistance with, feel free to share them. I'm here to help you achieve your goals and make your Personal-AI assistant a success.\n",
      "\n",
      "User: What do you know about my project?\n",
      "Bot:  Based on our previous conversations, I understand that you are working on a \"Personal-AI\" project using LangChain and OpenRouter with a JavaScript background. You are looking to incorporate natural language processing, conversational AI, and routing functionalities into your project. If there are specific goals or features you are aiming to achieve in your project, feel free to share them so I can provide more targeted assistance and guidance.\n"
     ]
    }
   ],
   "source": [
    "# Summary memory: keep a running summary + recent messages\n",
    "# Similar to compressing old log files while keeping recent ones\n",
    "\n",
    "summary = \"\"              # Running summary of old messages\n",
    "recent_messages = []       # Recent uncompressed messages\n",
    "SUMMARISE_AFTER = 6        # Summarise when we exceed this many messages\n",
    "\n",
    "summarise_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Summarise this conversation history concisely in 2-3 sentences.\"),\n",
    "    (\"human\", \"Previous summary: {summary}\\n\\nNew messages:\\n{messages}\")\n",
    "])\n",
    "\n",
    "def maybe_summarise():\n",
    "    \"\"\"Summarise if we have too many messages.\"\"\"\n",
    "    global summary, recent_messages\n",
    "    if len(recent_messages) >= SUMMARISE_AFTER:\n",
    "        messages_text = \"\\n\".join(\n",
    "            f\"{'User' if isinstance(m, HumanMessage) else 'Assistant'}: {m.content}\"\n",
    "            for m in recent_messages\n",
    "        )\n",
    "        new_summary = (summarise_prompt | llm | parser).invoke({\n",
    "            \"summary\": summary,\n",
    "            \"messages\": messages_text\n",
    "        })\n",
    "        print(f\"  üìù Summarised {len(recent_messages)} messages\")\n",
    "        summary = new_summary\n",
    "        recent_messages = []  # Clear recent messages after summarising\n",
    "\n",
    "def chat_with_summary(user_input: str) -> str:\n",
    "    \"\"\"Chat using summary + recent messages for context.\"\"\"\n",
    "    recent_messages.append(HumanMessage(content=user_input))\n",
    "\n",
    "    # Build context: system + summary (if any) + recent messages\n",
    "    context = [SystemMessage(content=\"You are a helpful assistant.\")]\n",
    "    if summary:\n",
    "        context.append(SystemMessage(content=f\"Conversation so far: {summary}\"))\n",
    "    context.extend(recent_messages)\n",
    "\n",
    "    response = llm.invoke(context)\n",
    "    recent_messages.append(AIMessage(content=response.content))\n",
    "\n",
    "    maybe_summarise()\n",
    "    return response.content\n",
    "\n",
    "# Test\n",
    "turns = [\n",
    "    \"I'm building an AI assistant with LangChain.\",\n",
    "    \"I'm using OpenRouter for the LLM provider.\",\n",
    "    \"The project is called personal-ai.\",\n",
    "    \"I started learning yesterday.\",\n",
    "    \"I come from a JavaScript background.\",\n",
    "    \"This is my 7th message.\",  # Should trigger summarisation\n",
    "    \"What do you know about my project?\"\n",
    "]\n",
    "\n",
    "for turn in turns:\n",
    "    print(f\"\\nUser: {turn}\")\n",
    "    print(f\"Bot:  {chat_with_summary(turn)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d1798b",
   "metadata": {},
   "source": [
    "## Part 5: Building a Stateful Chatbot\n",
    "\n",
    "Now let's bring everything together ‚Äî a reusable chatbot class with pluggable memory strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b3b49fd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: The Python equivalent of Array.map() is the map() function.\n",
      "Bot: The Python equivalent of Array.filter() is the filter() function.\n",
      "Bot: Yes, you can chain map() and filter() in Python using list comprehensions.\n",
      "\n",
      "üìú Conversation history:\n",
      "You : What's the Python equivalent of Array.map()?\n",
      "Bot : The Python equivalent of Array.map() is the map() function.\n",
      "You : And what about Array.filter()?\n",
      "Bot : The Python equivalent of Array.filter() is the filter() function.\n",
      "You : Can I chain them like in JS?\n",
      "Bot : Yes, you can chain map() and filter() in Python using list comprehensions.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "from collections import deque\n",
    "from typing import Literal\n",
    "\n",
    "class StatefulChatbot:\n",
    "    \"\"\"\n",
    "    A chatbot that remembers conversation history.\n",
    "    \n",
    "    memory_type: 'buffer' | 'window'\n",
    "      - 'buffer': keeps all messages (like an ever-growing array)\n",
    "      - 'window': keeps last k messages (like a circular buffer)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        system_prompt: str = \"You are a helpful assistant.\",\n",
    "        memory_type: Literal[\"buffer\", \"window\"] = \"window\",\n",
    "        window_size: int = 10,\n",
    "    ):\n",
    "        self.system_prompt = system_prompt\n",
    "        self.memory_type = memory_type\n",
    "\n",
    "        # Choose memory strategy\n",
    "        if memory_type == \"window\":\n",
    "            self.history = deque(maxlen=window_size)\n",
    "        else:\n",
    "            self.history = []\n",
    "\n",
    "        # Build the prompt template\n",
    "        self.prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", system_prompt),\n",
    "            MessagesPlaceholder(variable_name=\"history\"),\n",
    "            (\"human\", \"{input}\")\n",
    "        ])\n",
    "\n",
    "        self.chain = self.prompt | llm | parser\n",
    "\n",
    "    def chat(self, user_input: str) -> str:\n",
    "        \"\"\"Send a message and get a response.\"\"\"\n",
    "        response = self.chain.invoke({\n",
    "            \"history\": list(self.history),\n",
    "            \"input\": user_input\n",
    "        })\n",
    "\n",
    "        # Update history\n",
    "        self.history.append(HumanMessage(content=user_input))\n",
    "        self.history.append(AIMessage(content=response))\n",
    "\n",
    "        return response\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Clear conversation history.\"\"\"\n",
    "        if self.memory_type == \"window\":\n",
    "            self.history = deque(maxlen=self.history.maxlen)\n",
    "        else:\n",
    "            self.history = []\n",
    "        print(\"üóëÔ∏è  History cleared\")\n",
    "\n",
    "    def show_history(self):\n",
    "        \"\"\"Print conversation history.\"\"\"\n",
    "        for msg in self.history:\n",
    "            role = \"You \" if isinstance(msg, HumanMessage) else \"Bot \"\n",
    "            print(f\"{role}: {msg.content[:80]}...\" if len(msg.content) > 80 else f\"{role}: {msg.content}\")\n",
    "\n",
    "\n",
    "# --- Test it ---\n",
    "bot = StatefulChatbot(\n",
    "    system_prompt=\"You are a Python tutor for developers who know JavaScript. Be concise.\",\n",
    "    memory_type=\"window\",\n",
    "    window_size=10\n",
    ")\n",
    "\n",
    "print(\"Bot:\", bot.chat(\"What's the Python equivalent of Array.map()?\"))\n",
    "print(\"Bot:\", bot.chat(\"And what about Array.filter()?\"))\n",
    "print(\"Bot:\", bot.chat(\"Can I chain them like in JS?\"))\n",
    "\n",
    "print(\"\\nüìú Conversation history:\")\n",
    "bot.show_history()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40fbf5e",
   "metadata": {},
   "source": [
    "## üéØ Day 2 Exercises\n",
    "\n",
    "### Exercise 1: Multi-step Research Chain\n",
    "Build a sequential chain that:\n",
    "1. Takes a technology name as input\n",
    "2. Generates 3 key questions about it\n",
    "3. Answers all 3 questions\n",
    "4. Produces a concise summary\n",
    "\n",
    "### Exercise 2: Persona Chatbot\n",
    "Create a `StatefulChatbot` that acts as a specific persona (e.g. a senior engineer doing a code review). Test that it stays in character across multiple turns.\n",
    "\n",
    "### Exercise 3: Token Counter\n",
    "Extend `StatefulChatbot` to track approximate token usage per message (`len(text.split()) * 1.3` is a rough estimate). Print a warning when total tokens exceed 2000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5155c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91107170",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9e92d812",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total tokens used 22\n",
      "Bot: In Python, you can use list comprehension or the map() function to achieve similar functionality to Array.map() in JavaScript.\n",
      "total tokens used 53\n",
      "Bot: In Python, you can use list comprehension or the filter() function to achieve similar functionality to Array.filter() in JavaScript.\n",
      "total tokens used 87\n",
      "Bot: Yes, you can chain list comprehensions or function calls in Python like you would chain Array.map() and Array.filter() in JavaScript.\n",
      "\n",
      "üìú Conversation history:\n",
      "You : What's the Python equivalent of Array.map()?\n",
      "Bot : In Python, you can use list comprehension or the map() function to achieve simil...\n",
      "You : And what about Array.filter()?\n",
      "Bot : In Python, you can use list comprehension or the filter() function to achieve si...\n",
      "You : Can I chain them like in JS?\n",
      "Bot : Yes, you can chain list comprehensions or function calls in Python like you woul...\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "from collections import deque\n",
    "from typing import Literal\n",
    "\n",
    "class StatefulChatbot:\n",
    "    \"\"\"\n",
    "    A chatbot that remembers conversation history.\n",
    "    \n",
    "    memory_type: 'buffer' | 'window'\n",
    "      - 'buffer': keeps all messages (like an ever-growing array)\n",
    "      - 'window': keeps last k messages (like a circular buffer)\n",
    "    \"\"\"\n",
    "    token_limit = 2000  # Example token limit for context\n",
    "    def __init__(\n",
    "        self,\n",
    "        system_prompt: str = \"You are a helpful assistant.\",\n",
    "        memory_type: Literal[\"buffer\", \"window\"] = \"window\",\n",
    "        window_size: int = 10,\n",
    "    ):\n",
    "        self.system_prompt = system_prompt\n",
    "        self.memory_type = memory_type\n",
    "\n",
    "        # Choose memory strategy\n",
    "        if memory_type == \"window\":\n",
    "            self.history = deque(maxlen=window_size)\n",
    "        else:\n",
    "            self.history = []\n",
    "\n",
    "        # Build the prompt template\n",
    "        self.prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", system_prompt),\n",
    "            MessagesPlaceholder(variable_name=\"history\"),\n",
    "            (\"human\", \"{input}\")\n",
    "        ])\n",
    "\n",
    "        self.chain = self.prompt | llm | parser\n",
    "\n",
    "    def _estimate_tokens(self):\n",
    "        history_tokens = sum(len(msg.content.split()) for msg in self.history) * 1.3\n",
    "        system_tokens = len(self.system_prompt.split()) * 1.3\n",
    "        return int(history_tokens + system_tokens)\n",
    "\n",
    "\n",
    "    def chat(self, user_input: str) -> str:\n",
    "        \"\"\"Send a message and get a response.\"\"\"\n",
    "        # Check token limit\n",
    "        user_tokens = int(len(user_input.split())*1.3)\n",
    "        total = self._estimate_tokens() + user_tokens\n",
    "        print(f\"total tokens used {total}\")\n",
    "        if total > self.token_limit:\n",
    "            print('Exceeded token limit of 2000')\n",
    "\n",
    "        response = self.chain.invoke({\n",
    "            \"history\": list(self.history),\n",
    "            \"input\": user_input\n",
    "        })\n",
    "\n",
    "        # Update history\n",
    "        self.history.append(HumanMessage(content=user_input))\n",
    "        self.history.append(AIMessage(content=response))\n",
    "\n",
    "        return response\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Clear conversation history.\"\"\"\n",
    "        if self.memory_type == \"window\":\n",
    "            self.history = deque(maxlen=self.history.maxlen)\n",
    "        else:\n",
    "            self.history = []\n",
    "        print(\"üóëÔ∏è  History cleared\")\n",
    "\n",
    "    def show_history(self):\n",
    "        \"\"\"Print conversation history.\"\"\"\n",
    "        for msg in self.history:\n",
    "            role = \"You \" if isinstance(msg, HumanMessage) else \"Bot \"\n",
    "            print(f\"{role}: {msg.content[:80]}...\" if len(msg.content) > 80 else f\"{role}: {msg.content}\")\n",
    "\n",
    "\n",
    "# --- Test it ---\n",
    "bot = StatefulChatbot(\n",
    "    system_prompt=\"You are a Python tutor for developers who know JavaScript. Be concise.\",\n",
    "    memory_type=\"window\",\n",
    "    window_size=10\n",
    ")\n",
    "\n",
    "print(\"Bot:\", bot.chat(\"What's the Python equivalent of Array.map()?\"))\n",
    "print(\"Bot:\", bot.chat(\"And what about Array.filter()?\"))\n",
    "print(\"Bot:\", bot.chat(\"Can I chain them like in JS?\"))\n",
    "\n",
    "print(\"\\nüìú Conversation history:\")\n",
    "bot.show_history()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd06daa",
   "metadata": {},
   "source": [
    "## üìù Day 2 Summary\n",
    "\n",
    "**What you learned:**\n",
    "- ‚úÖ Sequential chains ‚Äî pipe output of one step into the next\n",
    "- ‚úÖ Parallel chains ‚Äî run multiple chains simultaneously (`RunnableParallel`)\n",
    "- ‚úÖ Router chains ‚Äî conditional logic to pick the right chain\n",
    "- ‚úÖ `MessagesPlaceholder` ‚Äî inject dynamic message lists into prompts\n",
    "- ‚úÖ Three memory strategies: buffer, window, summary\n",
    "- ‚úÖ Built a reusable `StatefulChatbot` class\n",
    "\n",
    "**Key concepts:**\n",
    "- **`StrOutputParser`**: Extracts plain string from LLM response\n",
    "- **`RunnablePassthrough`**: Passes input through unchanged (useful in parallel chains)\n",
    "- **`RunnableLambda`**: Wraps any Python function as a chain step\n",
    "- **`MessagesPlaceholder`**: The key to injecting chat history into a prompt template\n",
    "- **`deque(maxlen=k)`**: Python's built-in circular buffer ‚Äî perfect for sliding window memory\n",
    "\n",
    "**Next up (Day 3):**\n",
    "- Vector embeddings\n",
    "- Vector databases (ChromaDB)\n",
    "- RAG ‚Äî Retrieval Augmented Generation\n",
    "\n",
    "Great work! üéâ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
